{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', header=0, encoding='iso-8859-1')\n",
    "df_test = pd.read_csv('test.csv', header=0, encoding='iso-8859-1')\n",
    "df_att = pd.read_csv('attributes.csv', header=0, encoding='iso-8859-1')\n",
    "df_des = pd.read_csv('product_descriptions.csv', header=0, encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num of search term distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_term_analysis(df, comment):\n",
    "    search_terms = [search_term.split() for search_term in df.search_term]\n",
    "    num_words_used = [len(search_term) for search_term in search_terms]\n",
    "    avg_terms_used = sum(num_words_used) / float(len(search_terms))\n",
    "    print 'num_words_used: {}'.format(Counter(num_words_used))\n",
    "    print 'avg_terms_used: {}'.format(avg_terms_used)\n",
    "    \n",
    "    return search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words_used: Counter({3: 26575, 2: 18386, 4: 14847, 5: 6601, 1: 4503, 6: 2076, 7: 741, 8: 204, 9: 80, 11: 31, 10: 10, 12: 9, 14: 4})\n",
      "avg_terms_used: 3.15920720429\n"
     ]
    }
   ],
   "source": [
    "df_train.head()\n",
    "df_train.describe()\n",
    "search_terms_train = search_term_analysis(df_train, 'Training Data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words_used: Counter({3: 56420, 2: 51329, 4: 28100, 1: 12772, 5: 12004, 6: 4004, 7: 1241, 8: 469, 9: 145, 10: 104, 12: 54, 11: 46, 14: 4, 13: 1})\n",
      "avg_terms_used: 2.98237478478\n"
     ]
    }
   ],
   "source": [
    "df_test.head()\n",
    "search_terms_test = search_term_analysis(df_test, 'Training Data...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num of unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data - num of records: 74067, num unique products: 54667\n",
      "testing data - num of records: 166693, num unique products: 97460\n"
     ]
    }
   ],
   "source": [
    "print 'training data - num of records: {0}, num unique products: {1}'.format(len(df_train), len(set(df_train.product_uid)))\n",
    "print 'testing data - num of records: {0}, num unique products: {1}'.format(len(df_test), len(set(df_test.product_uid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by num of search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'microwaves'],\n",
       " [u'disposer'],\n",
       " [u'bamboo'],\n",
       " [u'grayson'],\n",
       " [u'post'],\n",
       " [u'outdoorfurniture'],\n",
       " [u'wiremesh'],\n",
       " [u'post'],\n",
       " [u'4x6'],\n",
       " [u'melnor'],\n",
       " [u'deckpaint'],\n",
       " [u'silicone'],\n",
       " [u'chipper'],\n",
       " [u'bidet'],\n",
       " [u'slatwall'],\n",
       " [u'respirator'],\n",
       " [u'lamp'],\n",
       " [u'Lawnmowers'],\n",
       " [u'closetmade'],\n",
       " [u'closetmaid'],\n",
       " [u'post'],\n",
       " [u'omnifilter'],\n",
       " [u'colosso'],\n",
       " [u'respirator'],\n",
       " [u'miricale'],\n",
       " [u'sheetrock'],\n",
       " [u'bathrooms'],\n",
       " [u'bootz'],\n",
       " [u'porcelain'],\n",
       " [u'aspiradora'],\n",
       " [u'shredder'],\n",
       " [u'chipper'],\n",
       " [u'Fuses'],\n",
       " [u'home-flex'],\n",
       " [u'knob'],\n",
       " [u'Lawnmowers'],\n",
       " [u'hindges'],\n",
       " [u'lathe'],\n",
       " [u'roller'],\n",
       " [u'clab'],\n",
       " [u'chaise'],\n",
       " [u'tent'],\n",
       " [u'cieling'],\n",
       " [u'gates'],\n",
       " [u'ceadar'],\n",
       " [u'pruners'],\n",
       " [u'barreir'],\n",
       " [u'water'],\n",
       " [u'azek'],\n",
       " [u'azek'],\n",
       " [u'closetmaid'],\n",
       " [u'stringer'],\n",
       " [u'thermoclear'],\n",
       " [u'stakes'],\n",
       " [u'slatwall'],\n",
       " [u'roller'],\n",
       " [u'slatwall'],\n",
       " [u'roller'],\n",
       " [u'huskvarna'],\n",
       " [u'post'],\n",
       " [u'plywoods'],\n",
       " [u'artificial'],\n",
       " [u'canopy'],\n",
       " [u'awnings'],\n",
       " [u'frp'],\n",
       " [u'rounds'],\n",
       " [u'round'],\n",
       " [u'rounds'],\n",
       " [u'closetmaid'],\n",
       " [u'sinks'],\n",
       " [u'roundup'],\n",
       " [u'sheetrock'],\n",
       " [u'spreader'],\n",
       " [u'tow'],\n",
       " [u'spreader'],\n",
       " [u'Lawnmowers'],\n",
       " [u'lamp'],\n",
       " [u'bathrooms'],\n",
       " [u'frp'],\n",
       " [u'tubs'],\n",
       " [u'briton'],\n",
       " [u'venner'],\n",
       " [u'barreir'],\n",
       " [u'durock'],\n",
       " [u'briton'],\n",
       " [u'spreader'],\n",
       " [u'lanterun'],\n",
       " [u'Fuses'],\n",
       " [u'tank'],\n",
       " [u'tent'],\n",
       " [u'roundup'],\n",
       " [u'stovetop'],\n",
       " [u'glasses'],\n",
       " [u'credenza'],\n",
       " [u'Fuses'],\n",
       " [u'tubs'],\n",
       " [u'rounds'],\n",
       " [u'roundup'],\n",
       " [u'waterton'],\n",
       " [u'cieling'],\n",
       " [u'brushless'],\n",
       " [u'paracord'],\n",
       " [u'paracord'],\n",
       " [u'doorbell'],\n",
       " [u'mtd'],\n",
       " [u'sinks'],\n",
       " [u'banbury'],\n",
       " [u'itwinkle'],\n",
       " [u'bidet'],\n",
       " [u'respirator'],\n",
       " [u'bateries'],\n",
       " [u'lockset'],\n",
       " [u'cieling'],\n",
       " [u'table'],\n",
       " [u'gates'],\n",
       " [u'doorbell'],\n",
       " [u'edsel'],\n",
       " [u'closetmaid'],\n",
       " [u'platforms'],\n",
       " [u'scaffoldings'],\n",
       " [u'spreader'],\n",
       " [u'construction'],\n",
       " [u'gates'],\n",
       " [u'pulley'],\n",
       " [u'hagchet'],\n",
       " [u'coveralls'],\n",
       " [u'labo'],\n",
       " [u'backsplach'],\n",
       " [u'standoffs'],\n",
       " [u'blowers'],\n",
       " [u'Lawnmowers'],\n",
       " [u'ion'],\n",
       " [u'closetmaid'],\n",
       " [u'lockset'],\n",
       " [u'spreader'],\n",
       " [u'melnor'],\n",
       " [u'blowers'],\n",
       " [u'makita'],\n",
       " [u'echo'],\n",
       " [u'chipper'],\n",
       " [u'standoffs'],\n",
       " [u'scaffolding'],\n",
       " [u'gates'],\n",
       " [u'microwaves'],\n",
       " [u'chairs'],\n",
       " [u'mdog'],\n",
       " [u'sheetrock'],\n",
       " [u'pulley'],\n",
       " [u'plywoods'],\n",
       " [u'echo'],\n",
       " [u'canopy'],\n",
       " [u'car'],\n",
       " [u'chime'],\n",
       " [u'ashwood'],\n",
       " [u'scod'],\n",
       " [u'lamp'],\n",
       " [u'mdog'],\n",
       " [u'microwaves'],\n",
       " [u'scaffolding'],\n",
       " [u'scaffoldings'],\n",
       " [u'scaffolds'],\n",
       " [u'weed'],\n",
       " [u'pruners'],\n",
       " [u'microwaves'],\n",
       " [u'marine'],\n",
       " [u'lathe'],\n",
       " [u'Fuses'],\n",
       " [u'forimca'],\n",
       " [u'outdoorfurniture'],\n",
       " [u'acrylic'],\n",
       " [u'slatwall'],\n",
       " [u'handtools'],\n",
       " [u'wheelbarow'],\n",
       " [u'wheelbarrows'],\n",
       " [u'echo'],\n",
       " [u'didger'],\n",
       " [u'quick'],\n",
       " [u'chairs'],\n",
       " [u'4x6'],\n",
       " [u'closetmaid'],\n",
       " [u'steqamers'],\n",
       " [u'azek'],\n",
       " [u'melnor'],\n",
       " [u'insallation'],\n",
       " [u'insullation'],\n",
       " [u'post'],\n",
       " [u'acrylic'],\n",
       " [u'desks'],\n",
       " [u'sjhelf'],\n",
       " [u'hecurles'],\n",
       " [u'tent'],\n",
       " [u'table'],\n",
       " [u'karcher'],\n",
       " [u'closetmade'],\n",
       " [u'closetmaid'],\n",
       " [u'1x8x8'],\n",
       " [u'drils'],\n",
       " [u'water'],\n",
       " [u'frp'],\n",
       " [u'wa'],\n",
       " [u'bended'],\n",
       " [u'Lawnmowers'],\n",
       " [u'cieling'],\n",
       " [u'ion'],\n",
       " [u'water'],\n",
       " [u'doorbell'],\n",
       " [u'scod'],\n",
       " [u'sheetrock'],\n",
       " [u'sauder'],\n",
       " [u'owens'],\n",
       " [u'acrylic'],\n",
       " [u'thermoclear'],\n",
       " [u'chisel'],\n",
       " [u'barreir'],\n",
       " [u'slatwall'],\n",
       " [u'chaise'],\n",
       " [u'chime'],\n",
       " [u'table'],\n",
       " [u'microwaves'],\n",
       " [u'4x6'],\n",
       " [u'post'],\n",
       " [u'paracord'],\n",
       " [u'tubs'],\n",
       " [u'table'],\n",
       " [u'tank'],\n",
       " [u'makita'],\n",
       " [u'lathe'],\n",
       " [u'echo'],\n",
       " [u'slatwall'],\n",
       " [u'scaffolding'],\n",
       " [u'insullation'],\n",
       " [u'owens'],\n",
       " [u'bolens'],\n",
       " [u'paintbrushes'],\n",
       " [u'4x6'],\n",
       " [u'azek'],\n",
       " [u'silicone'],\n",
       " [u'1x8x8'],\n",
       " [u'1x12x10'],\n",
       " [u'fountains'],\n",
       " [u'keg'],\n",
       " [u'pulley'],\n",
       " [u'echo'],\n",
       " [u'weed'],\n",
       " [u'microwaves'],\n",
       " [u'rumbl;estone'],\n",
       " [u'tar'],\n",
       " [u'paracord'],\n",
       " [u'lathe'],\n",
       " [u'insullation'],\n",
       " [u'melnor'],\n",
       " [u'frp'],\n",
       " [u'sheetrock'],\n",
       " [u'unsulation'],\n",
       " [u'upholstry'],\n",
       " [u'stakes'],\n",
       " [u'canopy'],\n",
       " [u'thermoplastic'],\n",
       " [u'chandeliers'],\n",
       " [u'gates'],\n",
       " [u'stringer'],\n",
       " [u'scaffolding'],\n",
       " [u'scaffolds'],\n",
       " [u'chime'],\n",
       " [u'acrylic'],\n",
       " [u'polycarbonite'],\n",
       " [u'table'],\n",
       " [u'tan'],\n",
       " [u'paracord'],\n",
       " [u'ion'],\n",
       " [u'taladros'],\n",
       " [u'azek'],\n",
       " [u'canopy'],\n",
       " [u'desks'],\n",
       " [u'Lawnmowers'],\n",
       " [u'knob'],\n",
       " [u'nobs'],\n",
       " [u'desks'],\n",
       " [u'chime'],\n",
       " [u'doorbell'],\n",
       " [u'echo'],\n",
       " [u'gas'],\n",
       " [u'chairs'],\n",
       " [u'Lawnmowers'],\n",
       " [u'microwaves'],\n",
       " [u'banbury'],\n",
       " [u'shopac'],\n",
       " [u'fountains'],\n",
       " [u'acrylic'],\n",
       " [u'silicone'],\n",
       " [u'acrylic'],\n",
       " [u'polycarbonite'],\n",
       " [u'parch'],\n",
       " [u'echo'],\n",
       " [u'pulley'],\n",
       " [u'shoplight'],\n",
       " [u'roundup'],\n",
       " [u'zeroturn'],\n",
       " [u'milwaukie'],\n",
       " [u'r-30'],\n",
       " [u'table'],\n",
       " [u'marine'],\n",
       " [u'blowers'],\n",
       " [u'fountains'],\n",
       " [u'technisoil'],\n",
       " [u'roundup'],\n",
       " [u'doorbell'],\n",
       " [u'frp'],\n",
       " [u'stakes'],\n",
       " [u'acrylic'],\n",
       " [u'polycarbonite'],\n",
       " [u'wet/dry'],\n",
       " [u'lanterun'],\n",
       " [u'bidet'],\n",
       " [u'chandeliers'],\n",
       " [u'roundup'],\n",
       " [u'respirator'],\n",
       " [u'chairs'],\n",
       " [u'lockset'],\n",
       " [u'Fuses'],\n",
       " [u'awnings'],\n",
       " [u'koolaroo'],\n",
       " [u'lamp'],\n",
       " [u'uplihght'],\n",
       " [u'table'],\n",
       " [u'echo'],\n",
       " [u'chipper'],\n",
       " [u'chisel'],\n",
       " [u'edsel'],\n",
       " [u'4x6'],\n",
       " [u'makita'],\n",
       " [u'awnings'],\n",
       " [u'slatwall'],\n",
       " [u'durock'],\n",
       " [u'respirator'],\n",
       " [u'paracord'],\n",
       " [u'karcher'],\n",
       " [u'roller'],\n",
       " [u'frp'],\n",
       " [u'coveralls'],\n",
       " [u'fountains'],\n",
       " [u'xps'],\n",
       " [u'stakes'],\n",
       " [u'bamboo'],\n",
       " [u'prices'],\n",
       " [u'post'],\n",
       " [u'chipper'],\n",
       " [u'acrylic'],\n",
       " [u'florida'],\n",
       " [u'hagchet'],\n",
       " [u'coveralls'],\n",
       " [u'taladros'],\n",
       " [u'4x6'],\n",
       " [u'Fuses'],\n",
       " [u'echo'],\n",
       " [u'sheetrock'],\n",
       " [u'bamboo'],\n",
       " [u'stakes'],\n",
       " [u'4x6'],\n",
       " [u'scaffolding'],\n",
       " [u'tresers'],\n",
       " [u'sheetrock'],\n",
       " [u'acrylic'],\n",
       " [u'extractor'],\n",
       " [u'silverton'],\n",
       " [u'durock'],\n",
       " [u'acrylic'],\n",
       " [u'drils'],\n",
       " [u'makita'],\n",
       " [u'lockset'],\n",
       " [u'step'],\n",
       " [u'chairs'],\n",
       " [u'closetmade'],\n",
       " [u'closetmaid'],\n",
       " [u'roundup'],\n",
       " [u'To'],\n",
       " [u'sinks'],\n",
       " [u'table'],\n",
       " [u'bamboo'],\n",
       " [u'spreader'],\n",
       " [u'melnor'],\n",
       " [u'lathe'],\n",
       " [u'closetmade'],\n",
       " [u'forimca'],\n",
       " [u'wiremesh'],\n",
       " [u'wheelbarow'],\n",
       " [u'platforms'],\n",
       " [u'awnings'],\n",
       " [u'insullation'],\n",
       " [u'azek'],\n",
       " [u'lockset'],\n",
       " [u'MASKING'],\n",
       " [u'bidet'],\n",
       " [u'1x8x8'],\n",
       " [u'fountains'],\n",
       " [u'sinks'],\n",
       " [u'rebarbender'],\n",
       " [u'table'],\n",
       " [u'chairs'],\n",
       " [u'stakes'],\n",
       " [u'plywoods'],\n",
       " [u'lockset'],\n",
       " [u'chairs'],\n",
       " [u'credenza'],\n",
       " [u'zeroturn'],\n",
       " [u'acrylic'],\n",
       " [u'silicone'],\n",
       " [u'roundup'],\n",
       " [u'tent'],\n",
       " [u'lamp'],\n",
       " [u'lockset'],\n",
       " [u'silicone'],\n",
       " [u'insallation'],\n",
       " [u'owens'],\n",
       " [u'storeage'],\n",
       " [u'lathe'],\n",
       " [u'tar'],\n",
       " [u'brushless'],\n",
       " [u'hagchet'],\n",
       " [u'chandeliers'],\n",
       " [u'lamp'],\n",
       " [u'winterizer'],\n",
       " [u'sheetrock'],\n",
       " [u'tubs'],\n",
       " [u'chime'],\n",
       " [u'roundup'],\n",
       " [u'makita'],\n",
       " [u'pulley'],\n",
       " [u'r-30'],\n",
       " [u'frp'],\n",
       " [u'acrylic'],\n",
       " [u'spreader'],\n",
       " [u'bamboo'],\n",
       " [u'banbury'],\n",
       " [u'forimca'],\n",
       " [u'laminated'],\n",
       " [u'post'],\n",
       " [u'chipper'],\n",
       " [u'mdog'],\n",
       " [u'roller'],\n",
       " [u'echo'],\n",
       " [u'rumbl;estone'],\n",
       " [u'chandeliers'],\n",
       " [u'workforce'],\n",
       " [u'canyon'],\n",
       " [u'durock'],\n",
       " [u'venner'],\n",
       " [u'versa'],\n",
       " [u'byefold'],\n",
       " [u'replacement'],\n",
       " [u'table'],\n",
       " [u'4x6'],\n",
       " [u'makita'],\n",
       " [u'gates'],\n",
       " [u'tent'],\n",
       " [u'Fuses'],\n",
       " [u'saud'],\n",
       " [u'gates'],\n",
       " [u'durock'],\n",
       " [u'tubs'],\n",
       " [u'lathe'],\n",
       " [u'Buff'],\n",
       " [u'fountains'],\n",
       " [u'chisel'],\n",
       " [u'bathrooms'],\n",
       " [u'slatwall'],\n",
       " [u'closetmaid'],\n",
       " [u'tent'],\n",
       " [u'scaffolding'],\n",
       " [u'awnings'],\n",
       " [u'bathrooms'],\n",
       " [u'shopac'],\n",
       " [u'chisel'],\n",
       " [u'closetmaid'],\n",
       " [u'bidet'],\n",
       " [u'doorbell'],\n",
       " [u'steqamers'],\n",
       " [u'beach'],\n",
       " [u'tubs'],\n",
       " [u'sinks'],\n",
       " [u'gates'],\n",
       " [u'line'],\n",
       " [u'edsel'],\n",
       " [u'nobs'],\n",
       " [u'bidet'],\n",
       " [u'hagchet'],\n",
       " [u'pulley'],\n",
       " [u'quick'],\n",
       " [u'spreader'],\n",
       " [u'canapu'],\n",
       " [u'Lawnmowers'],\n",
       " [u'table'],\n",
       " [u'lamp'],\n",
       " [u'gates'],\n",
       " [u'canopy'],\n",
       " [u'tent'],\n",
       " [u'tresers'],\n",
       " [u'silicone'],\n",
       " [u'shopac'],\n",
       " [u'doorbell'],\n",
       " [u'fountains'],\n",
       " [u'fountains'],\n",
       " [u'cat'],\n",
       " [u'chandeliers'],\n",
       " [u'inserts'],\n",
       " [u'post'],\n",
       " [u'8x8'],\n",
       " [u'post'],\n",
       " [u'inserts'],\n",
       " [u'roller'],\n",
       " [u'respirator'],\n",
       " [u'roller'],\n",
       " [u'roundup'],\n",
       " [u'spreader'],\n",
       " [u'Lawnmowers'],\n",
       " [u'shoplight'],\n",
       " [u'spreader'],\n",
       " [u'safavieh'],\n",
       " [u'closetmaid'],\n",
       " [u'scaffolding'],\n",
       " [u'lockset'],\n",
       " [u'car'],\n",
       " [u'outdoorfurniture'],\n",
       " [u'wndows'],\n",
       " [u'microwaves'],\n",
       " [u'bidet'],\n",
       " [u'hagchet'],\n",
       " [u'canopy'],\n",
       " [u'or'],\n",
       " [u'saud'],\n",
       " [u'silicone'],\n",
       " [u'bidet'],\n",
       " [u'chandeliers'],\n",
       " [u'milwaukie'],\n",
       " [u'pulley'],\n",
       " [u'chaise'],\n",
       " [u'pembria'],\n",
       " [u'thermoplastic'],\n",
       " [u'coveralls'],\n",
       " [u'slatwall'],\n",
       " [u'chisel'],\n",
       " [u'microwaves'],\n",
       " [u'bidet'],\n",
       " [u'afakro'],\n",
       " [u'deckpaint'],\n",
       " [u'chandeliers'],\n",
       " [u'durock'],\n",
       " [u'rumbl;estone'],\n",
       " [u'flea'],\n",
       " [u'acrylic'],\n",
       " [u'bidet'],\n",
       " [u'deckpaint'],\n",
       " [u'spreader'],\n",
       " [u'quick'],\n",
       " [u'pulley'],\n",
       " [u'Acurio'],\n",
       " [u'wastel'],\n",
       " [u'marrazi'],\n",
       " [u'chisel'],\n",
       " [u'makita'],\n",
       " [u'parch'],\n",
       " [u'chairs'],\n",
       " [u'scaffolding'],\n",
       " [u'scaffoldings'],\n",
       " [u'chipper'],\n",
       " [u'roundup'],\n",
       " [u'chandeliers'],\n",
       " [u'standoffs'],\n",
       " [u'closetmaid'],\n",
       " [u'sinks'],\n",
       " [u'awnings'],\n",
       " [u'coveralls'],\n",
       " [u'durock'],\n",
       " [u'tubs'],\n",
       " [u'closetmade'],\n",
       " [u'closetmaid'],\n",
       " [u'lathe'],\n",
       " [u'closetmaid'],\n",
       " [u'shelfa'],\n",
       " [u'sjhelf'],\n",
       " [u'linzer'],\n",
       " [u'sinks'],\n",
       " [u'ffill'],\n",
       " [u'insallation'],\n",
       " [u'owens'],\n",
       " [u'Fuses'],\n",
       " [u'ribbon'],\n",
       " [u'closetmaid'],\n",
       " [u'chandeliers'],\n",
       " [u'lathe'],\n",
       " [u'demon'],\n",
       " [u'saud'],\n",
       " [u'paracord'],\n",
       " [u'frp'],\n",
       " [u'gates'],\n",
       " [u'pulley'],\n",
       " [u'gas'],\n",
       " [u'linzer'],\n",
       " [u'paracord'],\n",
       " [u'Fuses'],\n",
       " [u'canopy'],\n",
       " [u'chandeliers'],\n",
       " [u'azek'],\n",
       " [u'azek'],\n",
       " [u'nobs'],\n",
       " [u'table'],\n",
       " [u'car'],\n",
       " [u'miricale'],\n",
       " [u'lockset'],\n",
       " [u'Lawnmowers'],\n",
       " [u'deckpaint'],\n",
       " [u'blowers'],\n",
       " [u'hindges'],\n",
       " [u'crawley'],\n",
       " [u'4x6'],\n",
       " [u'taladros'],\n",
       " [u'glasses'],\n",
       " [u'banbury'],\n",
       " [u'roundup'],\n",
       " [u'makita'],\n",
       " [u'extractor'],\n",
       " [u'warmer'],\n",
       " [u't-hinge'],\n",
       " [u'sofet'],\n",
       " [u'acclaim'],\n",
       " [u'tji'],\n",
       " [u'caladiums'],\n",
       " [u'Buff'],\n",
       " [u'platforms'],\n",
       " [u'scaffolds'],\n",
       " [u'adorne'],\n",
       " [u'5/4x10'],\n",
       " [u'couchen'],\n",
       " [u'pantries'],\n",
       " [u'chime'],\n",
       " [u'STEAMFAST'],\n",
       " [u'drive'],\n",
       " [u'wallcoverings'],\n",
       " [u'linzer'],\n",
       " [u'shelterlogic'],\n",
       " [u'lightsensor'],\n",
       " [u'tar'],\n",
       " [u'non-skid'],\n",
       " [u'fyrpon'],\n",
       " [u'plum'],\n",
       " [u'tank'],\n",
       " [u'toprail'],\n",
       " [u'YARDGUARD'],\n",
       " [u'tresers'],\n",
       " [u'SedgeHammer'],\n",
       " [u'luever'],\n",
       " [u'closer'],\n",
       " [u'melnor'],\n",
       " [u'rachet'],\n",
       " [u'FrankeUSA'],\n",
       " [u'non-skid'],\n",
       " [u'hindges'],\n",
       " [u'everlast'],\n",
       " [u'sharkbit'],\n",
       " [u'backsplach'],\n",
       " [u'milwaukie'],\n",
       " [u'8x8'],\n",
       " [u'dimmable'],\n",
       " [u'sjhelf'],\n",
       " [u'counter'],\n",
       " [u'wyndham'],\n",
       " [u'grayson'],\n",
       " [u'halogen'],\n",
       " [u'demon'],\n",
       " [u'mushrooms'],\n",
       " [u'ringer'],\n",
       " [u'augers'],\n",
       " [u'dimmable'],\n",
       " [u'grills-gas'],\n",
       " [u'counter'],\n",
       " [u'shanko'],\n",
       " [u'magnetic'],\n",
       " [u'2x4x18'],\n",
       " [u'polycarbonite'],\n",
       " [u'winterizer'],\n",
       " [u'forimca'],\n",
       " [u'brinkhill'],\n",
       " [u'koolaroo'],\n",
       " [u'airstone'],\n",
       " [u'greenwich'],\n",
       " [u'muriatic'],\n",
       " [u'dvr'],\n",
       " [u'b-vent'],\n",
       " [u'YARDGUARD'],\n",
       " [u'spaonges'],\n",
       " [u'wndows'],\n",
       " [u'igloo'],\n",
       " [u'closer'],\n",
       " [u'heat'],\n",
       " [u'prefab'],\n",
       " [u'bakewarte'],\n",
       " [u'koolaroo'],\n",
       " [u'rasp'],\n",
       " [u'line'],\n",
       " [u'mixer'],\n",
       " [u'fuacet'],\n",
       " [u'wringer'],\n",
       " [u'flyer'],\n",
       " [u'bosh'],\n",
       " [u'transormations'],\n",
       " [u'laurey'],\n",
       " [u'beach'],\n",
       " [u'carboy'],\n",
       " [u'extractor'],\n",
       " [u'refrigerant'],\n",
       " [u'muriatic'],\n",
       " [u'nylon'],\n",
       " [u'realtree'],\n",
       " [u'FrankeUSA'],\n",
       " [u'repel'],\n",
       " [u'huskvarna'],\n",
       " [u'aspiradora'],\n",
       " [u'vacume'],\n",
       " [u'wet/dry'],\n",
       " [u'igloo'],\n",
       " [u'mixer'],\n",
       " [u'futon'],\n",
       " [u'bernzomatic'],\n",
       " [u'Buff'],\n",
       " [u'bollard'],\n",
       " [u'pruners'],\n",
       " [u'lids'],\n",
       " [u'doorway'],\n",
       " [u'heat'],\n",
       " [u'b-vent'],\n",
       " [u'ffill'],\n",
       " [u'desks'],\n",
       " [u'chess'],\n",
       " [u'ROUGE'],\n",
       " [u'plywoods'],\n",
       " [u'dolls'],\n",
       " [u'2x6x14'],\n",
       " [u'pods'],\n",
       " [u'line'],\n",
       " [u'aspiradora'],\n",
       " [u'biscayne'],\n",
       " [u'2x2x6'],\n",
       " [u'flyer'],\n",
       " [u'wiremesh'],\n",
       " [u'sauder'],\n",
       " [u'drils'],\n",
       " [u'ornaments'],\n",
       " [u'knob'],\n",
       " [u'bathrooms'],\n",
       " [u'plywoods'],\n",
       " [u'tank'],\n",
       " [u'gnat'],\n",
       " [u'tan'],\n",
       " [u'FrankeUSA'],\n",
       " [u'bernzomatic'],\n",
       " [u'barns'],\n",
       " [u'adorne'],\n",
       " [u'adrone'],\n",
       " [u'riddex'],\n",
       " [u'step'],\n",
       " [u'madeline'],\n",
       " [u'POPPIES'],\n",
       " [u'adorne'],\n",
       " [u'adrone'],\n",
       " [u'lakeshore'],\n",
       " [u'flea'],\n",
       " [u'camping'],\n",
       " [u'laminet'],\n",
       " [u'sheffield'],\n",
       " [u'dolls'],\n",
       " [u'mixer'],\n",
       " [u'hedgers'],\n",
       " [u'memoirs'],\n",
       " [u'ibeam'],\n",
       " [u'peak'],\n",
       " [u'laminated'],\n",
       " [u'storeage'],\n",
       " [u'transormations'],\n",
       " [u'car'],\n",
       " [u'chime'],\n",
       " [u'emsco'],\n",
       " [u'deckpaint'],\n",
       " [u'gas'],\n",
       " [u'hudson'],\n",
       " [u'eureka'],\n",
       " [u'seals'],\n",
       " [u'toter'],\n",
       " [u'r-30'],\n",
       " [u'b-vent'],\n",
       " [u'homeline'],\n",
       " [u'chamois'],\n",
       " [u'Buff'],\n",
       " [u'handtools'],\n",
       " [u'bird-x'],\n",
       " [u'riddex'],\n",
       " [u'glasses'],\n",
       " [u'backsplach'],\n",
       " [u'greenwich'],\n",
       " [u'weed'],\n",
       " [u'or'],\n",
       " [u'alarm'],\n",
       " [u'surebond'],\n",
       " [u'bakewarte'],\n",
       " [u'pantries'],\n",
       " [u'8x8'],\n",
       " [u'grqss'],\n",
       " [u'sweep'],\n",
       " [u'silverton'],\n",
       " [u'pentas'],\n",
       " [u'candles'],\n",
       " [u'counter'],\n",
       " [u'wet/dry'],\n",
       " [u'or'],\n",
       " [u'phillits'],\n",
       " [u'camping'],\n",
       " [u'toprail'],\n",
       " [u'artificial'],\n",
       " [u'nut'],\n",
       " [u'21x21x1'],\n",
       " [u'medalion'],\n",
       " [u'To'],\n",
       " [u'shaw'],\n",
       " [u'chevron'],\n",
       " [u'showcase'],\n",
       " [u'car'],\n",
       " [u'exteria'],\n",
       " [u'outdoorfurniture'],\n",
       " [u'everlast'],\n",
       " [u'non-skid'],\n",
       " [u'artificial'],\n",
       " [u'symmons'],\n",
       " [u'toprail'],\n",
       " [u'halogen'],\n",
       " [u'coveralls'],\n",
       " [u'standoffs'],\n",
       " [u'shelterlogic'],\n",
       " [u'branches'],\n",
       " [u'flyer'],\n",
       " [u'augers'],\n",
       " [u'driveway'],\n",
       " [u'cr'],\n",
       " [u'valance'],\n",
       " [u'marrazi'],\n",
       " [u'midea'],\n",
       " [u'Buff'],\n",
       " [u'urethane'],\n",
       " [u'triplex'],\n",
       " [u'canyon'],\n",
       " [u'lakeshore'],\n",
       " [u'storeage'],\n",
       " [u'blenders'],\n",
       " [u'airstone'],\n",
       " [u'venner'],\n",
       " [u'ecosinks'],\n",
       " [u'polycarbonite'],\n",
       " [u'shakewood'],\n",
       " [u'kitchenfaucet'],\n",
       " [u'handtools'],\n",
       " [u'backsplach'],\n",
       " [u'omnifilter'],\n",
       " [u'adorne'],\n",
       " [u'staircase'],\n",
       " [u'boat'],\n",
       " [u'marine'],\n",
       " [u'unsulation'],\n",
       " [u'pediments'],\n",
       " [u'couchen'],\n",
       " [u'bird-x'],\n",
       " [u'Bunting'],\n",
       " [u'airblown'],\n",
       " [u'sticker'],\n",
       " [u'home-flex'],\n",
       " [u'shanko'],\n",
       " [u'manual'],\n",
       " [u'prefab'],\n",
       " [u'placemat'],\n",
       " [u'e-310'],\n",
       " [u'plumber'],\n",
       " [u'milwaukie'],\n",
       " [u'sawall'],\n",
       " [u'mastercool'],\n",
       " [u'crawley'],\n",
       " [u'ion'],\n",
       " [u'standoffs'],\n",
       " [u'candles'],\n",
       " [u'non-skid'],\n",
       " [u'sunshield'],\n",
       " [u'replacement'],\n",
       " [u'canapu'],\n",
       " [u'willow'],\n",
       " [u'Hearth'],\n",
       " [u'barns'],\n",
       " [u'pembria'],\n",
       " [u'shelterlogic'],\n",
       " [u'extractor'],\n",
       " [u'thermoplastic'],\n",
       " [u'dustpans'],\n",
       " [u'safety'],\n",
       " [u'tar'],\n",
       " [u'broiler'],\n",
       " [u'sharkbit'],\n",
       " [u'symmons'],\n",
       " [u'plywoods'],\n",
       " [u'fairy'],\n",
       " [u'downrod'],\n",
       " [u'monticello'],\n",
       " [u'heat'],\n",
       " [u'bug'],\n",
       " [u'valance'],\n",
       " [u'knaack'],\n",
       " [u'disposer'],\n",
       " [u'garde'],\n",
       " [u'acacia'],\n",
       " [u'canyon'],\n",
       " [u'wringer'],\n",
       " [u'margarita'],\n",
       " [u'backsplach'],\n",
       " [u'durham'],\n",
       " [u'ratchet'],\n",
       " [u'disposer'],\n",
       " [u'ridx'],\n",
       " [u'hudson'],\n",
       " [u'tank'],\n",
       " [u'sonicrafter'],\n",
       " [u'backsplach'],\n",
       " [u'rainbarrel'],\n",
       " [u'14x14'],\n",
       " [u'chamois'],\n",
       " [u'roxul'],\n",
       " [u'rasp'],\n",
       " [u'dvr'],\n",
       " [u'vikrell'],\n",
       " [u'platforms'],\n",
       " [u'gas'],\n",
       " [u'app'],\n",
       " [u'itwinkle'],\n",
       " [u'collapsible'],\n",
       " [u'sharkbit'],\n",
       " [u'mtd'],\n",
       " [u'parch'],\n",
       " [u'SedgeHammer'],\n",
       " [u'bandana'],\n",
       " [u'bernzomatic'],\n",
       " [u'dimmable'],\n",
       " [u'4*4'],\n",
       " [u'weman'],\n",
       " [u'daconil'],\n",
       " [u'shanko'],\n",
       " [u'valance'],\n",
       " [u'MASKING'],\n",
       " [u'quick'],\n",
       " [u'surebond'],\n",
       " [u'brayer'],\n",
       " [u'fen'],\n",
       " [u'rhododendrons'],\n",
       " [u'madeline'],\n",
       " [u'vanguard'],\n",
       " [u'YARDGUARD'],\n",
       " [u'igloo'],\n",
       " [u'barns'],\n",
       " [u'didger'],\n",
       " [u'tomostat'],\n",
       " [u'muriatic'],\n",
       " [u'pediments'],\n",
       " [u'downlight'],\n",
       " [u'wheelbarrows'],\n",
       " [u'bosh'],\n",
       " [u'baseboarders'],\n",
       " [u'pods'],\n",
       " [u'stairtreads'],\n",
       " [u'terminator'],\n",
       " [u'beech'],\n",
       " [u'artificial'],\n",
       " [u'bateries'],\n",
       " [u'throw'],\n",
       " [u'plywoods'],\n",
       " [u'surebond'],\n",
       " [u'extractor'],\n",
       " [u'greenhouses'],\n",
       " [u'SedgeHammer'],\n",
       " [u'shelfa'],\n",
       " [u'flexible'],\n",
       " [u'versa'],\n",
       " [u'bathrooms'],\n",
       " [u'STEAMFAST'],\n",
       " [u'medical'],\n",
       " [u'repel'],\n",
       " [u'flea'],\n",
       " [u'80/7'],\n",
       " [u'prices'],\n",
       " [u'aspiradora'],\n",
       " [u'gardenias'],\n",
       " [u'ceadar'],\n",
       " [u'cornerstone'],\n",
       " [u'12x12'],\n",
       " [u'bathrooms'],\n",
       " [u'alder'],\n",
       " [u'triplex'],\n",
       " [u'gardinias'],\n",
       " [u'lightsensor'],\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_by_num_words = defaultdict(list)\n",
    "for search_term in search_terms_train + search_terms_test:\n",
    "    search_by_num_words[len(search_term)].append(search_term)\n",
    "search_by_num_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train.relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge training and testing data sets to single dataframe\n",
    "\n",
    "df_train_copy = df_train.drop('relevance', axis=1)\n",
    "df_train_copy['dataset'] = 'train'\n",
    "\n",
    "df_test_copy = df_test.copy()\n",
    "df_test_copy['dataset'] = 'test'\n",
    "\n",
    "df = pd.concat([df_train_copy, df_test_copy], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are all products in product description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_uids = set(df_des.product_uid.values)\n",
    "len(filter(lambda uid: uid in unique_uids, df.product_uid.values)) / float(len(df.product_uid.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data with descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>dataset</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>train</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>train</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>train</td>\n",
       "      <td>BEHR Premium Textured DECKOVER is an innovativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>train</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>train</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "1   3       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "2   9       100002  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...   \n",
       "3  16       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "4  17       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "\n",
       "          search_term dataset  \\\n",
       "0       angle bracket   train   \n",
       "1           l bracket   train   \n",
       "2           deck over   train   \n",
       "3    rain shower head   train   \n",
       "4  shower only faucet   train   \n",
       "\n",
       "                                 product_description  \n",
       "0  Not only do angles make joints stronger, they ...  \n",
       "1  Not only do angles make joints stronger, they ...  \n",
       "2  BEHR Premium Textured DECKOVER is an innovativ...  \n",
       "3  Update your bathroom with the Delta Vero Singl...  \n",
       "4  Update your bathroom with the Delta Vero Singl...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_des, how='left', on='product_uid')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data with attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attributes_pivoted_df():\n",
    "\n",
    "    def filter_attributes(row):\n",
    "        if (row[0], row[1]) in filter_set:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    by_attribute = df_att.groupby(['product_uid', 'name']).count()\n",
    "    filter_set = by_attribute[by_attribute.value > 1].reset_index()[['product_uid', 'name']].values\n",
    "    filter_set = set([(uid, name) for uid, name in filter_set.tolist()])\n",
    "    \n",
    "    _df = df_att.copy()\n",
    "    _df['keep'] = df_att.apply(filter_attributes, axis=1)\n",
    "    _df = _df[_df['keep'] == 1]\n",
    "    _df = _df.dropna(how='any')\n",
    "    _df = _df.pivot(index='product_uid', columns='name', values='value')\n",
    "    _df.reset_index(inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivoted_attributes = get_attributes_pivoted_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# pattern_replace = []\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?)(-?)(in\\s|in\\.?\\s|inchs?|inches)(?i)', r'\\1inch '))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(ft\\.?|foot|feet)(?i)', r'\\1ft.'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(pounds?|lb\\.?)(?i)', r'\\1lb.'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(square|sq)(\\.?\\s?)(ft\\.?|foot|feet)(?i)', r'\\1sq.ft.'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(cubic|cu)(\\.?\\s?)(ft\\.?|foot|feet)(?i)', r'\\1cu.ft.'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(gallons?|gal\\.?)(?i)', r'\\1gal.'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(ounces?|ounce|oz\\.?)(?i)', r'\\1oz.'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(amperes?|amp\\.?s?)(?i)', r'\\1amp.'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(volt|volts)(?i)', r'\\1volts'))            \n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(watt|watts)(?i)', r'\\1watts'))\n",
    "# pattern_replace.append(('(\\d?_/?\\d)(\\s?-?)(mm\\.?|millimeter)(?i)', r'\\1mm'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(cm\\.?|centimeter)(?i)', r'\\1cm'))\n",
    "# pattern_replace.append(('(\\d?/?\\d)(\\s?-?)(m\\.?|meter)(?i)', r'\\1m'))\n",
    "# pattern_replace.append(('-', ' '))    \n",
    "# pattern_replace.append(('(\\(|\\))', ''))\n",
    "# pattern_replace.append(('refridgerator', 'fridge'))\n",
    "# pattern_replace.append(('refridgerator', 'fridge'))\n",
    "# pattern_replace.append(('(\\d)(\")', '\\1inch')) \n",
    "# pattern_replace.append((\"(\\d)(')\", '\\1ft.'))\n",
    "# pattern_replace.append((\" x \", ' x')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for (pattern, replace) in pattern_replace:\n",
    "    \n",
    "#     print 'pattern: {}'.format(pattern)\n",
    "#     print 'modified elements [search]: {}'.format(\n",
    "#         len([search for search in df1.search_term if re.search(pattern, search)]))\n",
    "\n",
    "#     print 'modified elements [product-title]: {}'.format(\n",
    "#         len([search for search in df1.product_title if re.search(pattern, search)]))\n",
    "        \n",
    "#     orig_rep_search = [(search, re.sub(pattern, replace, search)) \n",
    "#                        for search in df1.search_term if re.search(pattern, search)]\n",
    "\n",
    "#     orig_rep_title = [(product_title, re.sub(pattern, replace, product_title)) \n",
    "#                       for product_title in df1.product_title if re.search(pattern, product_title)]\n",
    "        \n",
    "#     df1.search_term = df1.search_term.map(lambda x: re.sub(pattern, replace, x))\n",
    "    \n",
    "#     df1.product_title = df1.product_title.map(lambda x: re.sub(pattern, replace, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import unicodedata\n",
    "strNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}\n",
    "\n",
    "def str_stem(s):\n",
    "    s = unicodedata.normalize('NFD', unicode(s)).encode('ascii', 'ignore')\n",
    "    s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    s = s.replace(\",\", \"\") #could be number / segment later\n",
    "    s = s.replace(\"$\", \" \")\n",
    "    s = s.replace(\"?\", \" \")\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = s.replace(\"//\", \"/\")\n",
    "    s = s.replace(\"..\", \".\")\n",
    "    s = s.replace(\" / \", \" \")\n",
    "    s = s.replace(\" \\\\ \", \" \")\n",
    "    s = s.replace(\".\", \" . \")\n",
    "    s = re.sub(r\"(^\\.|/)\", r\"\", s)\n",
    "    s = re.sub(r\"(\\.|/)$\", r\"\", s)\n",
    "    s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n",
    "    s = s.replace(\" x \", \" xbi \")\n",
    "    s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "    s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "    s = s.replace(\"*\", \" xbi \")\n",
    "    s = s.replace(\" by \", \" xbi \")\n",
    "    s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "    s = s.replace(\"Â°\", \" degrees \")\n",
    "    s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n",
    "    s = s.replace(\" v \", \" volts \")\n",
    "    s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    s = s.replace(\" . \", \" \")\n",
    "    s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n",
    "    s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"toliet\", \"toilet\")\n",
    "    s = s.replace(\"airconditioner\", \"air conditioner\")\n",
    "    s = s.replace(\"vinal\", \"vinyl\")\n",
    "    s = s.replace(\"vynal\", \"vinyl\")\n",
    "    s = s.replace(\"skill\", \"skil\")\n",
    "    s = s.replace(\"snowbl\", \"snow bl\")\n",
    "    s = s.replace(\"plexigla\", \"plexi gla\")\n",
    "    s = s.replace(\"rustoleum\", \"rust oleum\")\n",
    "    s = s.replace(\"whirpool\", \"whirlpool\")\n",
    "    s = s.replace(\"whirlpoolga\", \"whirlpool ga\")\n",
    "    s = s.replace(\"whirlpoolstainless\", \"whirlpool stainless\")\n",
    "    return s\n",
    "\n",
    "df1.search_term = df1.search_term.map(str_stem)\n",
    "df1.product_title = df1.product_title.map(str_stem)\n",
    "df_att.value = df_att.value.map(str_stem)\n",
    "# df1.product_description = df1.product_description.map(str_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis - TF-IDF + TSVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def lemma(word):\n",
    "    try:\n",
    "        return lmtzr.lemmatize(word.encode('ascii', 'ignore'))\n",
    "    except:\n",
    "        return word\n",
    "    \n",
    "def text_transformer(_df, _field):\n",
    "\n",
    "    text_array = _df[_field]\n",
    "    text_array_copy = text_array.map(lambda words: ' '.join([lemma(word) for word in words.split(' ')]))\n",
    "    \n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "    tsvd = TruncatedSVD(n_components=10, random_state=0)\n",
    "    \n",
    "    tfidf.fit(text_array)\n",
    "    mx_tfidf = tfidf.transform(text_array)\n",
    "    \n",
    "    tsvd.fit(mx_tfidf)\n",
    "    tsvd_tfidf = tsvd.transform(mx_tfidf)\n",
    "    \n",
    "    return tsvd_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_terms_ns = text_transformer(df1, 'search_term')\n",
    "product_titles_ns = text_transformer(df1, 'product_title')\n",
    "# product_description_ns = text_transformer(df1, 'product_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_search_terms = pd.DataFrame(search_terms_ns, index=df1.index, columns=['search_term_ns_' + str(x) \n",
    "                                                         for x in xrange(search_terms_ns.shape[1])])\n",
    "df_product_titles = pd.DataFrame(product_titles_ns, index=df1.index, columns=['product_titles_ns_' + str(x) \n",
    "                                                         for x in xrange(product_titles_ns.shape[1])])\n",
    "# df_product_descriptions = pd.DataFrame(product_description_ns, index=df1.index, columns=['product_description_ns_' + str(x) \n",
    "#                                                          for x in xrange(product_description_ns.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.concat([df1, df_search_terms, df_product_titles], axis=1, ignore_index=False)\n",
    "# df1 = pd.concat([df1, df_search_terms, df_product_titles, df_product_descriptions], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Score by Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms = []\n",
    "search_terms_cum_score = defaultdict(int)\n",
    "\n",
    "for search, score in zip(df1.search_term, y_train):\n",
    "    for term in map(lambda x: x.strip(), search.split(' ')):\n",
    "        search_terms.append(term)\n",
    "        search_terms_cum_score[term] += score\n",
    "        \n",
    "term_counts = sorted(Counter(search_terms).items(), key=lambda x: x[1], reverse=True)\n",
    "terms, counts = zip(*term_counts)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_score_by_term = dict()\n",
    "for term, count in zip(terms, counts):\n",
    "    mean_score_by_term[term] = search_terms_cum_score[term] / float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_score = np.mean(y_train)\n",
    "\n",
    "def term_avg_score(searches):\n",
    "    output_array = []\n",
    "    for search in searches:\n",
    "        cum_score = 0\n",
    "        term_count = 0\n",
    "        for term in map(lambda x: x.strip(), search.split(' ')):\n",
    "            if term in mean_score_by_term:\n",
    "                cum_score += mean_score_by_term[term]\n",
    "                term_count += 1\n",
    "        \n",
    "        if term_count > 0:\n",
    "            output_array.append(cum_score / float(term_count))\n",
    "        else:\n",
    "            output_array.append(baseline_score)\n",
    "        \n",
    "    return output_array\n",
    "\n",
    "df1['term_avg_score'] = term_avg_score(df1.search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# comment out after use\n",
    "# df1 = df1_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "stopwords_eng = set(stopwords.words(\"english\"))\n",
    "nan_arrays = []\n",
    "\n",
    "def ratio_words_matched(searches, match_phrases, ids, remove_stopwords=False, \n",
    "                        lemma=False, singularize=False, denominator='search'):\n",
    "    \n",
    "    output_array = []\n",
    "    for search_terms, match_phrase, _id in zip(searches, match_phrases, ids):\n",
    "        \n",
    "        if isinstance(match_phrase, float) and math.isnan(match_phrase):\n",
    "            nan_arrays.append((search_terms, match_phrase))\n",
    "            output_array.append(0)\n",
    "        else:\n",
    "            \n",
    "            a = search_terms\n",
    "            b = match_phrase\n",
    "            \n",
    "            try:\n",
    "                search_terms = search_terms.encode('ascii','ignore')\n",
    "                search_terms = str(search_terms)\n",
    "                search_terms = search_terms.lower()\n",
    "                \n",
    "                match_phrase = match_phrase.encode('ascii','ignore')\n",
    "                match_phrase = str(match_phrase)\n",
    "                match_phrase = match_phrase.lower()\n",
    "            except:\n",
    "                print 'error in encoding: {}, {}, {}'.format(search_terms, match_phrase, _id)\n",
    "                output_array.append(0)\n",
    "                pdb.set_trace()\n",
    "                continue\n",
    "            \n",
    "            if remove_stopwords:\n",
    "                search_terms = ' '.join([word for word in search_terms.split() if word not in stopwords_eng])\n",
    "                match_phrase = ' '.join([word for word in match_phrase.split() if word not in stopwords_eng])\n",
    "                \n",
    "            search_words = [term for term in search_terms.split() if term.strip() != '']\n",
    "            match_phrase_words = [term for term in match_phrase.split() if term.strip() != '']\n",
    "                     \n",
    "            if denominator == 'search':\n",
    "                num_matches = sum([1 for word in search_words if word in match_phrase_words])\n",
    "                \n",
    "                if len(search_words) > 0:\n",
    "                    output_array.append(num_matches / float(len(search_words))) \n",
    "                else:\n",
    "                    output_array.append(0)\n",
    "            else:\n",
    "                num_matches = sum([1 for word in match_phrase_words if word in search_words])\n",
    "                if len(search_words) > 0:\n",
    "                    output_array.append(num_matches / float(len(match_phrase_words))) \n",
    "                else:\n",
    "                    output_array.append(0)\n",
    "                \n",
    "    return output_array\n",
    "\n",
    "def num_chars_in_search(searches, remove_stopwords=False, remove_numeric_units=False):\n",
    "    \n",
    "    output_array = []\n",
    "    for search_terms in searches:\n",
    "        if remove_stopwords:\n",
    "            search_terms = ' '.join([word for word in search_terms.split() if word not in stopwords_eng])\n",
    "        if remove_numeric_units:\n",
    "            search_terms = ' '.join([word for word in search_terms.split() if word not in trivial_terms])\n",
    "        output_array.append(len(search_terms))\n",
    "    return output_array      \n",
    "\n",
    "def num_stopwords_in_search(searches):\n",
    "    output_array = []\n",
    "    for search_terms in searches:\n",
    "        _stopwords = [word for word in search_terms.split() if word in stopwords_eng]\n",
    "        output_array.append(len(_stopwords))\n",
    "    return output_array      \n",
    "\n",
    "def attribute_match(attribute):\n",
    "    \n",
    "    _df_merged = pd.merge(df1, df_pivoted_attributes[['product_uid', attribute]], how='left', on='product_uid')\n",
    "    output_array = ratio_words_matched(_df_merged.search_term, _df_merged[attribute], _df_merged.index)\n",
    "    \n",
    "    return [1 if x > 0 else 0 for x in output_array]\n",
    "\n",
    "def nth_word_matched(searches, match_phrases, n):\n",
    "    \n",
    "    if n == 0:\n",
    "        raise ValueError('input n must be greater than 0')\n",
    "    \n",
    "    stopwords_eng = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    output_array = []\n",
    "    for search_phrase, match_phrase in zip(searches, match_phrases):\n",
    "        \n",
    "        search_terms = search_phrase.split()\n",
    "        \n",
    "        # if there are n words\n",
    "        if n > len(search_terms) or search_terms[n-1] in stopwords_eng:\n",
    "            output_array.append(-1)\n",
    "        elif search_terms[n-1] in match_phrase:\n",
    "            output_array.append(1)\n",
    "        else:\n",
    "            output_array.append(0)\n",
    "    \n",
    "    print 'n: {}, counter: {}'.format(n, Counter(output_array))\n",
    "    return output_array      \n",
    "\n",
    "def word_matched(searches, match_phrases):\n",
    "    \n",
    "    output_array = []\n",
    "    stopwords_eng = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    for search, phrase in zip(searches, match_phrases):\n",
    "        last_word = search.split()[-1]\n",
    "        if last_word not in stopwords_eng and last_word in phrase.split():\n",
    "            output_array.append(1)\n",
    "        else:\n",
    "            output_array.append(0)\n",
    "        \n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5410"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pivoted_attributes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n"
     ]
    }
   ],
   "source": [
    "column_counts = []\n",
    "column_uniques = []\n",
    "count = 0\n",
    "for column in df_pivoted_attributes.columns:\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print count\n",
    "    \n",
    "    column_counts.append((column, \n",
    "                          df_pivoted_attributes[column].count(), \n",
    "                          len(df_pivoted_attributes[column].unique())))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Package Quantity', 6904, 282),\n",
       " (u'Bullet13', 6348, 2037),\n",
       " (u'Flooring Product Type', 6230, 93),\n",
       " (u'Color', 6214, 1314),\n",
       " (u'Tools Product Type', 6169, 14),\n",
       " (u'Included', 6079, 153),\n",
       " (u'Voltage (volts)', 6068, 105),\n",
       " (u'Assembly Required', 5718, 3),\n",
       " (u'Features', 5562, 530),\n",
       " (u'Wattage (watts)', 5107, 434),\n",
       " (u'Finish', 4996, 667),\n",
       " (u'Shape', 4876, 52),\n",
       " (u'Color/Finish Family', 4628, 74),\n",
       " (u'Electrical Product Type', 4409, 143),\n",
       " (u'Finish Family', 4209, 54),\n",
       " (u'Fixture Color/Finish', 4117, 764),\n",
       " (u'Product Thickness (in.)', 4080, 505),\n",
       " (u'Style', 4057, 32),\n",
       " (u'Interior/Exterior', 3950, 5),\n",
       " (u'Bullet14', 3853, 1447),\n",
       " (u'Number of Bulbs Required', 3802, 44),\n",
       " (u'Coverage Area (sq. ft.)', 3756, 276),\n",
       " (u'Finish Type', 3658, 18),\n",
       " (u'Power Tool Product Type', 3442, 20),\n",
       " (u'Paint Product Type', 3427, 114),\n",
       " (u'Outdoor Living Product Type', 3395, 127),\n",
       " (u'Collection Name', 3361, 765),\n",
       " (u'Hardware Finish Family', 3355, 23),\n",
       " (u'Bulb Type Included', 3331, 18),\n",
       " (u'Reconditioned', 3254, 3),\n",
       " (u'Light Source', 3204, 11),\n",
       " (u'Amperage (amps)', 3175, 281),\n",
       " (u'Paint/Stain Key Features', 3168, 97),\n",
       " (u'Container Size', 3128, 35),\n",
       " (u'Bulb Type', 3120, 19),\n",
       " (u'Bullet20', 3120, 41),\n",
       " (u'Dry to touch (min.)', 3117, 34),\n",
       " (u'Light Bulb Base Code', 2988, 42),\n",
       " (u'Fastener Type', 2982, 96),\n",
       " (u'Bullet15', 2913, 1063),\n",
       " (u'Bullet18', 2859, 238),\n",
       " (u'Product Thickness (mm)', 2823, 165),\n",
       " (u'Builders Hardware Product Type', 2785, 77),\n",
       " (u'Transparency', 2713, 6),\n",
       " (u'Paint/Stain Clean Up', 2686, 5),\n",
       " (u'Door Handing', 2533, 14),\n",
       " (u'Sheen', 2497, 10),\n",
       " (u'Weight Capacity (lb.)', 2497, 229),\n",
       " (u'Adjustable Lamp Head', 2485, 3),\n",
       " (u'Time before recoating (hours)', 2342, 29),\n",
       " (u'Appliance Type', 2322, 71),\n",
       " (u'Frame Material', 2290, 26),\n",
       " (u'Number of Doors', 2282, 8),\n",
       " (u'Fixture Color/Finish Family', 2256, 22),\n",
       " (u'Product Length (ft.)', 2241, 324),\n",
       " (u'Maximum Wattage (watts)', 2221, 62),\n",
       " (u'Weather Resistant', 2134, 3),\n",
       " (u'Door Type', 2099, 28),\n",
       " (u'Number of Pieces', 2061, 112),\n",
       " (u'Approximate Tile Size', 2036, 52),\n",
       " (u'Faucet type', 2019, 11),\n",
       " (u'Shade Color Family', 2006, 24),\n",
       " (u'Size', 2003, 116),\n",
       " (u'Decor Product Type', 2000, 20),\n",
       " (u'Application Type', 1975, 71),\n",
       " (u'Paintable/Stainable', 1955, 3),\n",
       " (u'RGB Value', 1937, 1126),\n",
       " (u'Flow rate (gallons per minute)', 1920, 58),\n",
       " (u'Bullet16', 1896, 716),\n",
       " (u'Kitchen Product Type', 1895, 102)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(column_counts, key=lambda x: x[1], reverse=True)[30:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort items to ranking of popularity common brands buckets\n",
    "def rank_by_count(attribute):\n",
    "    \n",
    "    rank_lookup = dict()\n",
    "    for i, (key, _) in enumerate(sorted(Counter(df_pivoted_attributes[attribute]).items(), \n",
    "                                        key=lambda x: x[1], reverse=True)):\n",
    "        rank_lookup[key] = i\n",
    "                                 \n",
    "    return rank_lookup                     \n",
    "\n",
    "# commoness of attributes\n",
    "rank_lookup_brand = rank_by_count('MFG Brand Name')\n",
    "rank_lookup_color = rank_by_count('Color Family')\n",
    "rank_lookup_material = rank_by_count('Material')\n",
    "rank_lookup_finish = rank_by_count('Color/Finish')\n",
    "\n",
    "df_pivoted_attributes['brand_c_index'] = df_pivoted_attributes['MFG Brand Name'].map(lambda x: rank_lookup_brand[x])\n",
    "df_pivoted_attributes['color_c_index'] = df_pivoted_attributes['Color Family'].map(lambda x: rank_lookup_color[x])\n",
    "df_pivoted_attributes['material_c_index'] = df_pivoted_attributes['Material'].map(lambda x: rank_lookup_material[x])\n",
    "df_pivoted_attributes['finish_c_index'] = df_pivoted_attributes['Color/Finish'].map(lambda x: rank_lookup_finish[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d90d94b59189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumpy_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/anselm/Library/Python/2.7/lib/python/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   2014\u001b[0m                                      index=self.index).__finalize__(self)\n\u001b[1;32m   2015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m             \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m             return self._constructor(mapped,\n\u001b[1;32m   2018\u001b[0m                                      index=self.index).__finalize__(self)\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:58435)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-d90d94b59189>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumpy_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "numpy_strings = np.array(df.search_term.map(lambda x: nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy_strings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter([word for words in numpy_strings for word in words]).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size ranking\n",
    "Counter(df_pivoted_attributes['Product Width (in.)']).most_common(100)\n",
    "Counter(df_pivoted_attributes['Product Height (in.)']).most_common(100)\n",
    "Counter(df_pivoted_attributes['Product Depth (in.)']).most_common(100)\n",
    "Counter(df_pivoted_attributes['Product Weight (lb.)']).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "print Counter(df_pivoted_attributes['Indoor/Outdoor']).most_common(20)\n",
    "print Counter(df_pivoted_attributes['Commercial / Residential']).most_common(20)\n",
    "(u'Assembly Required', 5718, 3),\n",
    "(u'Finish', 4996, 667),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1['num_chars_in_search'] = num_chars_in_search(df1.search_term, remove_stopwords=True)\n",
    "\n",
    "df1['ratio_words_matched_search'] = ratio_words_matched(df1.search_term, df1.product_title, df1.index,\n",
    "                                                        remove_stopwords=True, lemma=True)\n",
    "\n",
    "df1['ratio_words_matched_title'] = ratio_words_matched(df1.search_term, df1.product_title, df1.index,\n",
    "                                                       remove_stopwords=True, lemma=True, denominator='product_title')\n",
    "\n",
    "df1['num_stopwords_in_search'] = num_stopwords_in_search(df1.search_term)\n",
    "\n",
    "df1['brand_matched'] = attribute_match('MFG Brand Name')\n",
    "df1['material_matched'] = attribute_match('Material')\n",
    "df1['Bullet01_matched'] = attribute_match('Bullet01')\n",
    "df1['Bullet02_matched'] = attribute_match('Bullet02')\n",
    "df1['Bullet03_matched'] = attribute_match('Bullet03')\n",
    "df1['Bullet04_matched'] = attribute_match('Bullet04')\n",
    "df1['Bullet05_matched'] = attribute_match('Bullet05')\n",
    "df1['Bullet06_matched'] = attribute_match('Bullet06')\n",
    "df1['Bullet07_matched'] = attribute_match('Bullet07')\n",
    "df1['Bullet08_matched'] = attribute_match('Bullet08')\n",
    "df1['Bullet09_matched'] = attribute_match('Bullet09')\n",
    "df1['Bullet10_matched'] = attribute_match('Bullet10')\n",
    "df1['color_family_matched'] = attribute_match('Color Family')\n",
    "df1['color_finish_matched'] = attribute_match('Color/Finish')\n",
    "\n",
    "df1['first_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 1)\n",
    "df1['second_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 2)\n",
    "df1['third_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 3)\n",
    "df1['fourth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 4)\n",
    "df1['fifth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 5)\n",
    "df1['sixth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 6)\n",
    "df1['seventh_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 7)\n",
    "df1['eighth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 8)\n",
    "\n",
    "df1['query_last_word_in_title'] = word_matched(df1.search_term, df1.product_title)\n",
    "df1['query_last_word_in_description'] = word_matched(df1.search_term, df1.product_description)\n",
    "\n",
    "# df1['num_tdidf_words'] = num_tdidf_words(df1.search_term)\n",
    "# sorted(Counter(df_pivoted_attributes['MFG Brand Name']).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "def pos_tag_count(searches):\n",
    "    word_pos = []\n",
    "    \n",
    "    count = 0    \n",
    "    for search_phrase in searches:\n",
    "        count += 1\n",
    "        if count % 50000 == 0:\n",
    "            print count \n",
    "            \n",
    "        word_pos.append(nltk.tag._pos_tag(search_phrase, None, tagger))\n",
    "    \n",
    "    return word_pos\n",
    "\n",
    "word_pos = pos_tag_count(df1.search_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_list = map(lambda word_list: [x[1] for x in word_list], word_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_all = []\n",
    "for pos in pos_list:\n",
    "    pos_all.extend(pos)\n",
    "\n",
    "pos_counts = Counter(pos_all)\n",
    "for pos, _ in pos_counts.iteritems():\n",
    "    df1['pos_' + str(pos)] = [x.count(pos) for x in pos_list]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_backup = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: n-gram matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up values for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_x(_df, features=None, feature_indices=None):\n",
    "    if features:\n",
    "        _df_train = _df[features][_df.dataset == 'train']\n",
    "        _df_test = _df[features][_df.dataset == 'test']\n",
    "    elif feature_indices:\n",
    "        _df = df.iloc[:, features_indices]\n",
    "        _df_train = _df[features][_df.dataset == 'train']\n",
    "        _df_test = _df[features][_df.dataset == 'test']\n",
    "    else:\n",
    "        _df_train = _df[_df.dataset == 'train']\n",
    "        _df_test = _df[_df.dataset == 'test']\n",
    "        \n",
    "        _df_train.drop(['dataset'], axis=1, inplace=True)\n",
    "        _df_test.drop(['dataset'], axis=1, inplace=True)\n",
    "        \n",
    "    return _df_train.values, _df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features = [u'term_avg_score', u'ratio_words_matched_search',\n",
    "       u'ratio_words_matched_title', u'query_last_word_in_title',\n",
    "       u'product_titles_ns_3', u'product_titles_ns_1', u'product_titles_ns_4',\n",
    "       u'product_titles_ns_6', u'product_titles_ns_0', u'product_titles_ns_7',\n",
    "       u'product_titles_ns_9', u'product_titles_ns_5', u'product_titles_ns_2',\n",
    "       u'product_titles_ns_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# df1 = df_copy.copy()\n",
    "# df_copy = df1.copy()\n",
    "\n",
    "df1 = df1.drop(['product_uid', 'product_title', 'search_term', 'product_description'], axis=1)\n",
    "# x_train, x_test = get_train_test_x(df1, features = selected_features)\n",
    "x_train, x_test = get_train_test_x(df1)\n",
    "df1 = df1.drop(['dataset'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn import pipeline, grid_search\n",
    "\n",
    "def RMSE(y, y_pred):\n",
    "    return round(mean_squared_error(y, y_pred)**0.5, 3)\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "RMSE_scorer = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "import sklearn.preprocessing as pp\n",
    "le = pp.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(random_state = 0, verbose = 0)\n",
    "param_grid = {'n_estimators':[30], 'max_features': [10], 'max_depth': [10]}\n",
    "model_rfr = grid_search.GridSearchCV(estimator = rfr, param_grid = param_grid, \n",
    "                                     cv = 2, verbose = 20, scoring=RMSE_scorer, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_model(_model, _x_train, _y_train):\n",
    "    _model.fit(_x_train, _y_train)\n",
    "\n",
    "    print(\"Best parameters found by grid search:\")\n",
    "    print(_model.best_params_)\n",
    "    print(\"Best CV score:\")\n",
    "    print(_model.best_score_)\n",
    "\n",
    "    return _model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_optimized = run_model(model_rfr, x_train_copy, y_train_copy)\n",
    "# Full Feature Set\n",
    "# -0.449\n",
    "\n",
    "# {'max_features': 20, 'n_estimators': 800, 'max_depth': 20}\n",
    "# Best CV score:\n",
    "# -0.448834649641\n",
    "\n",
    "# 0.451271 before ensemble\n",
    "\n",
    "model_optimized = run_model(model_rfr, x_train, y_train)\n",
    "# Full Feature Set\n",
    "# -0.4521\n",
    "\n",
    "# {'max_features': 20, 'n_estimators': 800, 'max_depth': 20}\n",
    "# Best CV score:\n",
    "# -0.44682674871\n",
    "\n",
    "# 0.454765 before ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_x_train = x_train\n",
    "\n",
    "ith_e = 0\n",
    "ensemble_iterations = 5\n",
    "while ith_e < ensemble_iterations:\n",
    "    curr_x_train = new_x_train\n",
    "    \n",
    "    ith_e += 1\n",
    "    print 'ith Ensemble Iteration: {}'.format(ith_e)\n",
    "    \n",
    "    model_optimized = run_model(model_rfr, curr_x_train, y_train)\n",
    "    output = model_optimized.predict(curr_x_train)\n",
    "    \n",
    "    new_x_train = np.zeros((curr_x_train.shape[0], curr_x_train.shape[1] + 1))\n",
    "    new_x_train[:,:-1] = curr_x_train\n",
    "    new_x_train[:,-1] = output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_optimized = run_model(model_optimized, new_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "xgbr = XGBRegressor(seed=0)\n",
    "param_grid = {'objective':['reg:linear', 'reg:logistic'], \n",
    "              'n_estimators': [50], 'max_depth': [5], \n",
    "              'learning_rate': [0.01, 0.1]}\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = xgbr, param_grid = param_grid, cv = 2, \n",
    "                                 verbose = 20, scoring=RMSE_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_copy = x_train \n",
    "y_train_copy = y_train\n",
    "\n",
    "# Best parameters found by grid search:\n",
    "# {'max_features': 5, 'n_estimators': 600, 'max_depth': 20}\n",
    "# Best CV score:\n",
    "# -0.447705321577"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "# from itertools import product\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# def run_kfold_process(clf, labels, partial_train=1):\n",
    "    \n",
    "#     if partial_train < 1:\n",
    "#         y_partial_train = y_train[:int(len(y_train) * partial_train)]\n",
    "#     else:\n",
    "#         y_partial_train = y_train\n",
    "        \n",
    "#     folds = 3\n",
    "#     kfold = StratifiedKFold(y=y_partial_train, n_folds=folds, shuffle=True, random_state=0)        \n",
    "#     predictions_all = []\n",
    "    \n",
    "#     cum_score = 0\n",
    "#     cum_score_train = 0\n",
    "\n",
    "#     for k, (train, test) in enumerate(kfold):\n",
    "        \n",
    "#         clf.fit(x_train[train], labels[train])\n",
    "        \n",
    "#         output_train = clf.predict(x_train[train])\n",
    "#         output = clf.predict(x_train[test])\n",
    "            \n",
    "#         cum_score_train += RMSE(y_train[train], output_train) # not applied to classification yet\n",
    "#         cum_score += RMSE(y_train[test], output)\n",
    "\n",
    "#         predictions_all.append(output)\n",
    "\n",
    "#     rmse_train = cum_score_train / float(folds)\n",
    "#     rmse = cum_score / float(folds)\n",
    "\n",
    "#     print 'Avg. RMSE (test): {}'.format(round(rmse,4))\n",
    "#     print 'Avg. RMSE (train): {}'.format(round(rmse_train,4))\n",
    "    \n",
    "#     return predictions_all\n",
    "\n",
    "# def train_clf(clf, partial_train=1):    \n",
    "\n",
    "#     labels = y_train\n",
    "#     predictions_all = run_kfold_process(clf, labels, partial_train=partial_train)\n",
    "        \n",
    "#     return predictions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# param_n_estimators = [20, 30, 50, 80, 120, 200]\n",
    "# param_max_depth = [10]\n",
    "# param_max_features = [20]\n",
    "# partial_factors = [1]\n",
    "\n",
    "# for n_estimators, max_depth, max_features, partial_factor in itertools.product(\n",
    "#     param_n_estimators, param_max_depth, param_max_features, partial_factors):\n",
    "    \n",
    "#     print 'n_estimators: {}, max_depth: {}, max_features: {}, partial_train: {}'.format(\n",
    "#         n_estimators, max_depth, max_features, partial_factor)\n",
    "    \n",
    "#     rfr = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, \n",
    "#                                 max_features=max_features, random_state=0)\n",
    "    \n",
    "#     predictions_all = train_clf(rfr, partial_train=partial_factor)\n",
    "\n",
    "# # 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# param_n_estimators = [30]\n",
    "# partial_factors = [0.01, 0.1, 0.2, 0.5, 1]\n",
    "# rfr = RandomForestRegressor(n_estimators=20, max_depth=10, max_features=20, random_state=0)\n",
    "\n",
    "# for n_estimators, partial_factor in itertools.product(param_n_estimators, partial_factors):\n",
    "#     print 'n_estimators: {}, partial_train: {}'.format(n_estimators, partial_factor)\n",
    "#     br = BaggingRegressor(rfr, n_estimators=n_estimators, random_state=0)\n",
    "#     predictions_all = train_clf(br, partial_train=partial_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# import itertools\n",
    "\n",
    "# param_n_estimators = [50]\n",
    "# param_learning_rate = [0.1]\n",
    "# partial_factors = [0.01, 0.1, 0.2, 0.5, 1]\n",
    "# for n_estimators, learning_rate, partial_factor in itertools.product(param_n_estimators, \n",
    "#                                                                      param_learning_rate, \n",
    "#                                                                      partial_factors):\n",
    "    \n",
    "#     print 'n_estimators: {}, learning_rate: {}, partial_factor: {}'.format(n_estimators, learning_rate, partial_factor)\n",
    "#     abr = AdaBoostRegressor(rfr, n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "#     predictions_all = train_clf(abr, partial_train=partial_factor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# # param_max_depth = [1, 5, 10]\n",
    "# # param_n_estimators = [1, 5, 10, 20]\n",
    "# # param_learning_rate = [0.001, 0.01, 0.1]\n",
    "# param_max_depth = [10]\n",
    "# param_max_features = [20]\n",
    "# param_n_estimators = [30]\n",
    "# param_learning_rate = [0.001, 0.01, 0.1, 1, 10]\n",
    "# partial_factors = [1]\n",
    "# for max_depth, n_estimators, learning_rate, partial_factor in itertools.product(param_max_depth,\n",
    "#                                                                                 param_max_features,\n",
    "#                                                                                 param_n_estimators, \n",
    "#                                                                                 param_learning_rate,\n",
    "#                                                                                 partial_factors):\n",
    "    \n",
    "#     print 'max_depth: {}, max_features: {}, n_estimators: {}, learning_rate: {}, partial_factor: {}'.format(\n",
    "#         max_depth, max_features, n_estimators, learning_rate, partial_factor)\n",
    "#     gbr = GradientBoostingRegressor(max_depth=max_depth,\n",
    "#                                     max_features=max_features,\n",
    "#                                     n_estimators=n_estimators, \n",
    "#                                     learning_rate=learning_rate, \n",
    "#                                     random_state=0)\n",
    "#     predictions_all = train_clf(gbr, partial_train=partial_factor)\n",
    "\n",
    "# # 0.534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# param_n_estimators = [40, 80, 120]\n",
    "# param_learning_rate = [0.05, 0.1, 0.25]\n",
    "# params_max_depth = [10]\n",
    "# # param_max_features = [20]\n",
    "# # partial_factors = [0.1, 0.5, 1]\n",
    "# partial_factors = [1]\n",
    "\n",
    "# for n_estimators, learning_rate, max_depth, partial_factor in itertools.product(\n",
    "#     param_n_estimators, param_learning_rate, params_max_depth, partial_factors):\n",
    "    \n",
    "#     print 'n_estimators: {}, max_depth:{}, learning_rate: {}, partial_factor: {}'.format(\n",
    "#         n_estimators, max_depth, learning_rate, partial_factor)\n",
    "    \n",
    "#     xgbr = XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, \n",
    "#                         reg_alpha=1, seed=0)\n",
    "    \n",
    "#     predictions_all = train_clf(xgbr, partial_train=partial_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print abr.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print map(lambda x: round(x, 2), sorted(model_optimized.feature_importances_)[::-1])\n",
    "print df1.columns[np.argsort(model_optimized.feature_importances_)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_best_k_features(k):\n",
    "    return np.argsort(model_optimized.feature_importances_)[::-1][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(predictions_all[0], 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print df1.columns\n",
    "np.around(np.corrcoef(x_train, rowvar=0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(x_train)\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydotplus\n",
    "dot_data = StringIO() \n",
    "\n",
    "import graphviz\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "for i, _tree in enumerate(rfc.estimators_):\n",
    "    with open('figures/tree_' + str(i) + '.dot', 'w') as dotfile:\n",
    "        dot_data = StringIO() \n",
    "        tree.export_graphviz(_tree, out_file=dot_data)\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "        \n",
    "#         graph = pydotplus.graph_from_dot_file('tree_0.dot') \n",
    "#         graph.write_pdf(\"tree.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_x_train = x_train\n",
    "new_x_test = x_test\n",
    "\n",
    "rfr = RandomForestRegressor(random_state = 0, verbose = 0)\n",
    "param_grid = {'n_estimators':[500], 'max_features': [15,25], 'max_depth': [15,25]}\n",
    "model_rfr = grid_search.GridSearchCV(estimator = rfr, param_grid = param_grid, \n",
    "                                     cv = 2, verbose = 20, scoring=RMSE_scorer, n_jobs=-1)\n",
    "\n",
    "ith_e = 0\n",
    "ensemble_iterations = 1\n",
    "while ith_e < ensemble_iterations:\n",
    "    ith_e += 1\n",
    "    curr_x_train = new_x_train\n",
    "    curr_x_test = new_x_test\n",
    "    \n",
    "    model_optimized = run_model(model_rfr, curr_x_train, y_train)\n",
    "    output_train = model_optimized.predict(curr_x_train)\n",
    "\n",
    "    new_x_train = np.zeros((curr_x_train.shape[0], curr_x_train.shape[1]+1))\n",
    "    new_x_train[:,:-1] = curr_x_train\n",
    "    new_x_train[:,-1] = output_train\n",
    "\n",
    "    output_test = model_optimized.predict(curr_x_test)\n",
    "    new_x_test = np.zeros((curr_x_test.shape[0], curr_x_test.shape[1]+1))\n",
    "    new_x_test[:,:-1] = curr_x_test\n",
    "    new_x_test[:,-1] = output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # rfr = RandomForestRegressor(max_depth=10, max_features=20, n_estimators=50, random_state=0)\n",
    "# model_optimized.fit(new_x_train, y_train)\n",
    "# preds = model_optimized.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "with open('results/results_rfr_ensemble.csv', 'wb') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['id', 'relevance'])\n",
    "    for id, pred in zip(df_test.id.values, preds):\n",
    "        csv_writer.writerow([id, pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
