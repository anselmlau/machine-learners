{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "HomeDepot Kaggle Competition: Goal is to predict the search term relevance by the user inputted search team and returned product. Training data is provided in the form of search terms, products and a manually curated score. These scores are the labels that the model needs to predict.\n",
    "\n",
    "The dataset is preprocessed using the NLTK library and basic NLP techniques such as stemming. Features are engineered using data transformations such as tf-idf and truncated SVD - this is to reduce the dimensionality of the feature set. A Random Forest Regressor is used as the final model predictor due to its ability to minimize outlier effects and non-parametric property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', header=0, encoding='iso-8859-1')\n",
    "df_test = pd.read_csv('test.csv', header=0, encoding='iso-8859-1')\n",
    "df_att = pd.read_csv('attributes.csv', header=0, encoding='iso-8859-1')\n",
    "df_des = pd.read_csv('product_descriptions.csv', header=0, encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Num of search term distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_term_analysis(df, comment):\n",
    "    search_terms = [search_term.split() for search_term in df.search_term]\n",
    "    num_words_used = [len(search_term) for search_term in search_terms]\n",
    "    avg_terms_used = sum(num_words_used) / float(len(search_terms))\n",
    "    print 'num_words_used: {}'.format(Counter(num_words_used))\n",
    "    print 'avg_terms_used: {}'.format(avg_terms_used)\n",
    "    \n",
    "    return search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words_used: Counter({3: 26575, 2: 18386, 4: 14847, 5: 6601, 1: 4503, 6: 2076, 7: 741, 8: 204, 9: 80, 11: 31, 10: 10, 12: 9, 14: 4})\n",
      "avg_terms_used: 3.15920720429\n"
     ]
    }
   ],
   "source": [
    "df_train.head()\n",
    "df_train.describe()\n",
    "search_terms_train = search_term_analysis(df_train, 'Training Data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words_used: Counter({3: 56420, 2: 51329, 4: 28100, 1: 12772, 5: 12004, 6: 4004, 7: 1241, 8: 469, 9: 145, 10: 104, 12: 54, 11: 46, 14: 4, 13: 1})\n",
      "avg_terms_used: 2.98237478478\n"
     ]
    }
   ],
   "source": [
    "df_test.head()\n",
    "search_terms_test = search_term_analysis(df_test, 'Training Data...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Num of unique products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data - num of records: 74067, num unique products: 54667\n",
      "testing data - num of records: 166693, num unique products: 97460\n"
     ]
    }
   ],
   "source": [
    "print 'training data - num of records: {0}, num unique products: {1}'.format(len(df_train), len(set(df_train.product_uid)))\n",
    "print 'testing data - num of records: {0}, num unique products: {1}'.format(len(df_test), len(set(df_test.product_uid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by num of search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_by_num_words = defaultdict(list)\n",
    "for search_term in search_terms_train + search_terms_test:\n",
    "    search_by_num_words[len(search_term)].append(search_term)\n",
    "search_by_num_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train.relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge training and testing data sets to single dataframe\n",
    "\n",
    "df_train_copy = df_train.drop('relevance', axis=1)\n",
    "df_train_copy['dataset'] = 'train'\n",
    "\n",
    "df_test_copy = df_test.copy()\n",
    "df_test_copy['dataset'] = 'test'\n",
    "\n",
    "df = pd.concat([df_train_copy, df_test_copy], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are all products in product description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_uids = set(df_des.product_uid.values)\n",
    "len(filter(lambda uid: uid in unique_uids, df.product_uid.values)) / float(len(df.product_uid.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data with descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>dataset</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>train</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>train</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>train</td>\n",
       "      <td>BEHR Premium Textured DECKOVER is an innovativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>train</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>train</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "1   3       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "2   9       100002  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...   \n",
       "3  16       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "4  17       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "\n",
       "          search_term dataset  \\\n",
       "0       angle bracket   train   \n",
       "1           l bracket   train   \n",
       "2           deck over   train   \n",
       "3    rain shower head   train   \n",
       "4  shower only faucet   train   \n",
       "\n",
       "                                 product_description  \n",
       "0  Not only do angles make joints stronger, they ...  \n",
       "1  Not only do angles make joints stronger, they ...  \n",
       "2  BEHR Premium Textured DECKOVER is an innovativ...  \n",
       "3  Update your bathroom with the Delta Vero Singl...  \n",
       "4  Update your bathroom with the Delta Vero Singl...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, df_des, how='left', on='product_uid')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data with attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attributes_pivoted_df():\n",
    "\n",
    "    def filter_attributes(row):\n",
    "        if (row[0], row[1]) in filter_set:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    by_attribute = df_att.groupby(['product_uid', 'name']).count()\n",
    "    filter_set = by_attribute[by_attribute.value > 1].reset_index()[['product_uid', 'name']].values\n",
    "    filter_set = set([(uid, name) for uid, name in filter_set.tolist()])\n",
    "    \n",
    "    _df = df_att.copy()\n",
    "    _df['keep'] = df_att.apply(filter_attributes, axis=1)\n",
    "    _df = _df[_df['keep'] == 1]\n",
    "    _df = _df.dropna(how='any')\n",
    "    _df = _df.pivot(index='product_uid', columns='name', values='value')\n",
    "    _df.reset_index(inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pivoted_attributes = get_attributes_pivoted_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import unicodedata\n",
    "strNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}\n",
    "\n",
    "def str_stem(s):\n",
    "    s = unicodedata.normalize('NFD', unicode(s)).encode('ascii', 'ignore')\n",
    "    s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    s = s.replace(\",\", \"\") #could be number / segment later\n",
    "    s = s.replace(\"$\", \" \")\n",
    "    s = s.replace(\"?\", \" \")\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = s.replace(\"//\", \"/\")\n",
    "    s = s.replace(\"..\", \".\")\n",
    "    s = s.replace(\" / \", \" \")\n",
    "    s = s.replace(\" \\\\ \", \" \")\n",
    "    s = s.replace(\".\", \" . \")\n",
    "    s = re.sub(r\"(^\\.|/)\", r\"\", s)\n",
    "    s = re.sub(r\"(\\.|/)$\", r\"\", s)\n",
    "    s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n",
    "    s = s.replace(\" x \", \" xbi \")\n",
    "    s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "    s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "    s = s.replace(\"*\", \" xbi \")\n",
    "    s = s.replace(\" by \", \" xbi \")\n",
    "    s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "    s = s.replace(\"Â°\", \" degrees \")\n",
    "    s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n",
    "    s = s.replace(\" v \", \" volts \")\n",
    "    s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    s = s.replace(\" . \", \" \")\n",
    "    s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n",
    "    s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"toliet\", \"toilet\")\n",
    "    s = s.replace(\"airconditioner\", \"air conditioner\")\n",
    "    s = s.replace(\"vinal\", \"vinyl\")\n",
    "    s = s.replace(\"vynal\", \"vinyl\")\n",
    "    s = s.replace(\"skill\", \"skil\")\n",
    "    s = s.replace(\"snowbl\", \"snow bl\")\n",
    "    s = s.replace(\"plexigla\", \"plexi gla\")\n",
    "    s = s.replace(\"rustoleum\", \"rust oleum\")\n",
    "    s = s.replace(\"whirpool\", \"whirlpool\")\n",
    "    s = s.replace(\"whirlpoolga\", \"whirlpool ga\")\n",
    "    s = s.replace(\"whirlpoolstainless\", \"whirlpool stainless\")\n",
    "    return s\n",
    "\n",
    "df1.search_term = df1.search_term.map(str_stem)\n",
    "df1.product_title = df1.product_title.map(str_stem)\n",
    "df_att.value = df_att.value.map(str_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis - TF-IDF + TSVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def lemma(word):\n",
    "    try:\n",
    "        return lmtzr.lemmatize(word.encode('ascii', 'ignore'))\n",
    "    except:\n",
    "        return word\n",
    "    \n",
    "def text_transformer(_df, _field):\n",
    "\n",
    "    text_array = _df[_field]\n",
    "    text_array_copy = text_array.map(lambda words: ' '.join([lemma(word) for word in words.split(' ')]))\n",
    "    \n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "    tsvd = TruncatedSVD(n_components=10, random_state=0)\n",
    "    \n",
    "    tfidf.fit(text_array)\n",
    "    mx_tfidf = tfidf.transform(text_array)\n",
    "    \n",
    "    tsvd.fit(mx_tfidf)\n",
    "    tsvd_tfidf = tsvd.transform(mx_tfidf)\n",
    "    \n",
    "    return tsvd_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_terms_ns = text_transformer(df1, 'search_term')\n",
    "product_titles_ns = text_transformer(df1, 'product_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_search_terms = pd.DataFrame(search_terms_ns, index=df1.index, columns=['search_term_ns_' + str(x) \n",
    "                                                         for x in xrange(search_terms_ns.shape[1])])\n",
    "df_product_titles = pd.DataFrame(product_titles_ns, index=df1.index, columns=['product_titles_ns_' + str(x) \n",
    "                                                         for x in xrange(product_titles_ns.shape[1])])\n",
    "df1 = pd.concat([df1, df_search_terms, df_product_titles], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Score by Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms = []\n",
    "search_terms_cum_score = defaultdict(int)\n",
    "\n",
    "for search, score in zip(df1.search_term, y_train):\n",
    "    for term in map(lambda x: x.strip(), search.split(' ')):\n",
    "        search_terms.append(term)\n",
    "        search_terms_cum_score[term] += score\n",
    "        \n",
    "term_counts = sorted(Counter(search_terms).items(), key=lambda x: x[1], reverse=True)\n",
    "terms, counts = zip(*term_counts)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_score_by_term = dict()\n",
    "for term, count in zip(terms, counts):\n",
    "    mean_score_by_term[term] = search_terms_cum_score[term] / float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_score = np.mean(y_train)\n",
    "\n",
    "def term_avg_score(searches):\n",
    "    output_array = []\n",
    "    for search in searches:\n",
    "        cum_score = 0\n",
    "        term_count = 0\n",
    "        for term in map(lambda x: x.strip(), search.split(' ')):\n",
    "            if term in mean_score_by_term:\n",
    "                cum_score += mean_score_by_term[term]\n",
    "                term_count += 1\n",
    "        \n",
    "        if term_count > 0:\n",
    "            output_array.append(cum_score / float(term_count))\n",
    "        else:\n",
    "            output_array.append(baseline_score)\n",
    "        \n",
    "    return output_array\n",
    "\n",
    "df1['term_avg_score'] = term_avg_score(df1.search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "stopwords_eng = set(stopwords.words(\"english\"))\n",
    "nan_arrays = []\n",
    "\n",
    "def ratio_words_matched(searches, match_phrases, ids, remove_stopwords=False, \n",
    "                        lemma=False, singularize=False, denominator='search'):\n",
    "    \n",
    "    output_array = []\n",
    "    for search_terms, match_phrase, _id in zip(searches, match_phrases, ids):\n",
    "        \n",
    "        if isinstance(match_phrase, float) and math.isnan(match_phrase):\n",
    "            nan_arrays.append((search_terms, match_phrase))\n",
    "            output_array.append(0)\n",
    "        else:\n",
    "            \n",
    "            a = search_terms\n",
    "            b = match_phrase\n",
    "            \n",
    "            try:\n",
    "                search_terms = search_terms.encode('ascii','ignore')\n",
    "                search_terms = str(search_terms)\n",
    "                search_terms = search_terms.lower()\n",
    "                \n",
    "                match_phrase = match_phrase.encode('ascii','ignore')\n",
    "                match_phrase = str(match_phrase)\n",
    "                match_phrase = match_phrase.lower()\n",
    "            except:\n",
    "                print 'error in encoding: {}, {}, {}'.format(search_terms, match_phrase, _id)\n",
    "                output_array.append(0)\n",
    "                pdb.set_trace()\n",
    "                continue\n",
    "            \n",
    "            if remove_stopwords:\n",
    "                search_terms = ' '.join([word for word in search_terms.split() if word not in stopwords_eng])\n",
    "                match_phrase = ' '.join([word for word in match_phrase.split() if word not in stopwords_eng])\n",
    "                \n",
    "            search_words = [term for term in search_terms.split() if term.strip() != '']\n",
    "            match_phrase_words = [term for term in match_phrase.split() if term.strip() != '']\n",
    "                     \n",
    "            if denominator == 'search':\n",
    "                num_matches = sum([1 for word in search_words if word in match_phrase_words])\n",
    "                \n",
    "                if len(search_words) > 0:\n",
    "                    output_array.append(num_matches / float(len(search_words))) \n",
    "                else:\n",
    "                    output_array.append(0)\n",
    "            else:\n",
    "                num_matches = sum([1 for word in match_phrase_words if word in search_words])\n",
    "                if len(search_words) > 0:\n",
    "                    output_array.append(num_matches / float(len(match_phrase_words))) \n",
    "                else:\n",
    "                    output_array.append(0)\n",
    "                \n",
    "    return output_array\n",
    "\n",
    "def num_chars_in_search(searches, remove_stopwords=False, remove_numeric_units=False):\n",
    "    \n",
    "    output_array = []\n",
    "    for search_terms in searches:\n",
    "        if remove_stopwords:\n",
    "            search_terms = ' '.join([word for word in search_terms.split() if word not in stopwords_eng])\n",
    "        if remove_numeric_units:\n",
    "            search_terms = ' '.join([word for word in search_terms.split() if word not in trivial_terms])\n",
    "        output_array.append(len(search_terms))\n",
    "    return output_array      \n",
    "\n",
    "def num_stopwords_in_search(searches):\n",
    "    output_array = []\n",
    "    for search_terms in searches:\n",
    "        _stopwords = [word for word in search_terms.split() if word in stopwords_eng]\n",
    "        output_array.append(len(_stopwords))\n",
    "    return output_array      \n",
    "\n",
    "def attribute_match(attribute):\n",
    "    \n",
    "    _df_merged = pd.merge(df1, df_pivoted_attributes[['product_uid', attribute]], how='left', on='product_uid')\n",
    "    output_array = ratio_words_matched(_df_merged.search_term, _df_merged[attribute], _df_merged.index)\n",
    "    \n",
    "    return [1 if x > 0 else 0 for x in output_array]\n",
    "\n",
    "def nth_word_matched(searches, match_phrases, n):\n",
    "    \n",
    "    if n == 0:\n",
    "        raise ValueError('input n must be greater than 0')\n",
    "    \n",
    "    stopwords_eng = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    output_array = []\n",
    "    for search_phrase, match_phrase in zip(searches, match_phrases):\n",
    "        \n",
    "        search_terms = search_phrase.split()\n",
    "        \n",
    "        # if there are n words\n",
    "        if n > len(search_terms) or search_terms[n-1] in stopwords_eng:\n",
    "            output_array.append(-1)\n",
    "        elif search_terms[n-1] in match_phrase:\n",
    "            output_array.append(1)\n",
    "        else:\n",
    "            output_array.append(0)\n",
    "    \n",
    "    print 'n: {}, counter: {}'.format(n, Counter(output_array))\n",
    "    return output_array      \n",
    "\n",
    "def word_matched(searches, match_phrases):\n",
    "    \n",
    "    output_array = []\n",
    "    stopwords_eng = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    for search, phrase in zip(searches, match_phrases):\n",
    "        last_word = search.split()[-1]\n",
    "        if last_word not in stopwords_eng and last_word in phrase.split():\n",
    "            output_array.append(1)\n",
    "        else:\n",
    "            output_array.append(0)\n",
    "        \n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5410"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pivoted_attributes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_counts = []\n",
    "column_uniques = []\n",
    "count = 0\n",
    "for column in df_pivoted_attributes.columns:\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print count\n",
    "    \n",
    "    column_counts.append((column, \n",
    "                          df_pivoted_attributes[column].count(), \n",
    "                          len(df_pivoted_attributes[column].unique())))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Package Quantity', 6904, 282),\n",
       " (u'Bullet13', 6348, 2037),\n",
       " (u'Flooring Product Type', 6230, 93),\n",
       " (u'Color', 6214, 1314),\n",
       " (u'Tools Product Type', 6169, 14),\n",
       " (u'Included', 6079, 153),\n",
       " (u'Voltage (volts)', 6068, 105),\n",
       " (u'Assembly Required', 5718, 3),\n",
       " (u'Features', 5562, 530),\n",
       " (u'Wattage (watts)', 5107, 434),\n",
       " (u'Finish', 4996, 667),\n",
       " (u'Shape', 4876, 52),\n",
       " (u'Color/Finish Family', 4628, 74),\n",
       " (u'Electrical Product Type', 4409, 143),\n",
       " (u'Finish Family', 4209, 54),\n",
       " (u'Fixture Color/Finish', 4117, 764),\n",
       " (u'Product Thickness (in.)', 4080, 505),\n",
       " (u'Style', 4057, 32),\n",
       " (u'Interior/Exterior', 3950, 5),\n",
       " (u'Bullet14', 3853, 1447),\n",
       " (u'Number of Bulbs Required', 3802, 44),\n",
       " (u'Coverage Area (sq. ft.)', 3756, 276),\n",
       " (u'Finish Type', 3658, 18),\n",
       " (u'Power Tool Product Type', 3442, 20),\n",
       " (u'Paint Product Type', 3427, 114),\n",
       " (u'Outdoor Living Product Type', 3395, 127),\n",
       " (u'Collection Name', 3361, 765),\n",
       " (u'Hardware Finish Family', 3355, 23),\n",
       " (u'Bulb Type Included', 3331, 18),\n",
       " (u'Reconditioned', 3254, 3),\n",
       " (u'Light Source', 3204, 11),\n",
       " (u'Amperage (amps)', 3175, 281),\n",
       " (u'Paint/Stain Key Features', 3168, 97),\n",
       " (u'Container Size', 3128, 35),\n",
       " (u'Bulb Type', 3120, 19),\n",
       " (u'Bullet20', 3120, 41),\n",
       " (u'Dry to touch (min.)', 3117, 34),\n",
       " (u'Light Bulb Base Code', 2988, 42),\n",
       " (u'Fastener Type', 2982, 96),\n",
       " (u'Bullet15', 2913, 1063),\n",
       " (u'Bullet18', 2859, 238),\n",
       " (u'Product Thickness (mm)', 2823, 165),\n",
       " (u'Builders Hardware Product Type', 2785, 77),\n",
       " (u'Transparency', 2713, 6),\n",
       " (u'Paint/Stain Clean Up', 2686, 5),\n",
       " (u'Door Handing', 2533, 14),\n",
       " (u'Sheen', 2497, 10),\n",
       " (u'Weight Capacity (lb.)', 2497, 229),\n",
       " (u'Adjustable Lamp Head', 2485, 3),\n",
       " (u'Time before recoating (hours)', 2342, 29),\n",
       " (u'Appliance Type', 2322, 71),\n",
       " (u'Frame Material', 2290, 26),\n",
       " (u'Number of Doors', 2282, 8),\n",
       " (u'Fixture Color/Finish Family', 2256, 22),\n",
       " (u'Product Length (ft.)', 2241, 324),\n",
       " (u'Maximum Wattage (watts)', 2221, 62),\n",
       " (u'Weather Resistant', 2134, 3),\n",
       " (u'Door Type', 2099, 28),\n",
       " (u'Number of Pieces', 2061, 112),\n",
       " (u'Approximate Tile Size', 2036, 52),\n",
       " (u'Faucet type', 2019, 11),\n",
       " (u'Shade Color Family', 2006, 24),\n",
       " (u'Size', 2003, 116),\n",
       " (u'Decor Product Type', 2000, 20),\n",
       " (u'Application Type', 1975, 71),\n",
       " (u'Paintable/Stainable', 1955, 3),\n",
       " (u'RGB Value', 1937, 1126),\n",
       " (u'Flow rate (gallons per minute)', 1920, 58),\n",
       " (u'Bullet16', 1896, 716),\n",
       " (u'Kitchen Product Type', 1895, 102)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(column_counts, key=lambda x: x[1], reverse=True)[30:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort items to ranking of popularity common brands buckets\n",
    "def rank_by_count(attribute):\n",
    "    \n",
    "    rank_lookup = dict()\n",
    "    for i, (key, _) in enumerate(sorted(Counter(df_pivoted_attributes[attribute]).items(), \n",
    "                                        key=lambda x: x[1], reverse=True)):\n",
    "        rank_lookup[key] = i\n",
    "                                 \n",
    "    return rank_lookup                     \n",
    "\n",
    "# commoness of attributes\n",
    "rank_lookup_brand = rank_by_count('MFG Brand Name')\n",
    "rank_lookup_color = rank_by_count('Color Family')\n",
    "rank_lookup_material = rank_by_count('Material')\n",
    "rank_lookup_finish = rank_by_count('Color/Finish')\n",
    "\n",
    "df_pivoted_attributes['brand_c_index'] = df_pivoted_attributes['MFG Brand Name'].map(lambda x: rank_lookup_brand[x])\n",
    "df_pivoted_attributes['color_c_index'] = df_pivoted_attributes['Color Family'].map(lambda x: rank_lookup_color[x])\n",
    "df_pivoted_attributes['material_c_index'] = df_pivoted_attributes['Material'].map(lambda x: rank_lookup_material[x])\n",
    "df_pivoted_attributes['finish_c_index'] = df_pivoted_attributes['Color/Finish'].map(lambda x: rank_lookup_finish[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy_strings = np.array(df.search_term.map(lambda x: nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter([word for words in numpy_strings for word in words]).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size ranking\n",
    "Counter(df_pivoted_attributes['Product Width (in.)']).most_common(100)\n",
    "Counter(df_pivoted_attributes['Product Height (in.)']).most_common(100)\n",
    "Counter(df_pivoted_attributes['Product Depth (in.)']).most_common(100)\n",
    "Counter(df_pivoted_attributes['Product Weight (lb.)']).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "print Counter(df_pivoted_attributes['Indoor/Outdoor']).most_common(20)\n",
    "print Counter(df_pivoted_attributes['Commercial / Residential']).most_common(20)\n",
    "(u'Assembly Required', 5718, 3),\n",
    "(u'Finish', 4996, 667),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1['num_chars_in_search'] = num_chars_in_search(df1.search_term, remove_stopwords=True)\n",
    "\n",
    "df1['ratio_words_matched_search'] = ratio_words_matched(df1.search_term, df1.product_title, df1.index,\n",
    "                                                        remove_stopwords=True, lemma=True)\n",
    "\n",
    "df1['ratio_words_matched_title'] = ratio_words_matched(df1.search_term, df1.product_title, df1.index,\n",
    "                                                       remove_stopwords=True, lemma=True, denominator='product_title')\n",
    "\n",
    "df1['num_stopwords_in_search'] = num_stopwords_in_search(df1.search_term)\n",
    "\n",
    "df1['brand_matched'] = attribute_match('MFG Brand Name')\n",
    "df1['material_matched'] = attribute_match('Material')\n",
    "df1['Bullet01_matched'] = attribute_match('Bullet01')\n",
    "df1['Bullet02_matched'] = attribute_match('Bullet02')\n",
    "df1['Bullet03_matched'] = attribute_match('Bullet03')\n",
    "df1['Bullet04_matched'] = attribute_match('Bullet04')\n",
    "df1['Bullet05_matched'] = attribute_match('Bullet05')\n",
    "df1['Bullet06_matched'] = attribute_match('Bullet06')\n",
    "df1['Bullet07_matched'] = attribute_match('Bullet07')\n",
    "df1['Bullet08_matched'] = attribute_match('Bullet08')\n",
    "df1['Bullet09_matched'] = attribute_match('Bullet09')\n",
    "df1['Bullet10_matched'] = attribute_match('Bullet10')\n",
    "df1['color_family_matched'] = attribute_match('Color Family')\n",
    "df1['color_finish_matched'] = attribute_match('Color/Finish')\n",
    "\n",
    "df1['first_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 1)\n",
    "df1['second_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 2)\n",
    "df1['third_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 3)\n",
    "df1['fourth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 4)\n",
    "df1['fifth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 5)\n",
    "df1['sixth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 6)\n",
    "df1['seventh_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 7)\n",
    "df1['eighth_word_matched'] = nth_word_matched(df1.search_term, df1.product_title, 8)\n",
    "\n",
    "df1['query_last_word_in_title'] = word_matched(df1.search_term, df1.product_title)\n",
    "df1['query_last_word_in_description'] = word_matched(df1.search_term, df1.product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "def pos_tag_count(searches):\n",
    "    word_pos = []\n",
    "    \n",
    "    count = 0    \n",
    "    for search_phrase in searches:\n",
    "        count += 1\n",
    "        if count % 50000 == 0:\n",
    "            print count \n",
    "            \n",
    "        word_pos.append(nltk.tag._pos_tag(search_phrase, None, tagger))\n",
    "    \n",
    "    return word_pos\n",
    "\n",
    "word_pos = pos_tag_count(df1.search_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_list = map(lambda word_list: [x[1] for x in word_list], word_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_all = []\n",
    "for pos in pos_list:\n",
    "    pos_all.extend(pos)\n",
    "\n",
    "pos_counts = Counter(pos_all)\n",
    "for pos, _ in pos_counts.iteritems():\n",
    "    df1['pos_' + str(pos)] = [x.count(pos) for x in pos_list]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_backup = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up values for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_x(_df, features=None, feature_indices=None):\n",
    "    if features:\n",
    "        _df_train = _df[features][_df.dataset == 'train']\n",
    "        _df_test = _df[features][_df.dataset == 'test']\n",
    "    elif feature_indices:\n",
    "        _df = df.iloc[:, features_indices]\n",
    "        _df_train = _df[features][_df.dataset == 'train']\n",
    "        _df_test = _df[features][_df.dataset == 'test']\n",
    "    else:\n",
    "        _df_train = _df[_df.dataset == 'train']\n",
    "        _df_test = _df[_df.dataset == 'test']\n",
    "        \n",
    "        _df_train.drop(['dataset'], axis=1, inplace=True)\n",
    "        _df_test.drop(['dataset'], axis=1, inplace=True)\n",
    "        \n",
    "    return _df_train.values, _df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features = [u'term_avg_score', u'ratio_words_matched_search',\n",
    "       u'ratio_words_matched_title', u'query_last_word_in_title',\n",
    "       u'product_titles_ns_3', u'product_titles_ns_1', u'product_titles_ns_4',\n",
    "       u'product_titles_ns_6', u'product_titles_ns_0', u'product_titles_ns_7',\n",
    "       u'product_titles_ns_9', u'product_titles_ns_5', u'product_titles_ns_2',\n",
    "       u'product_titles_ns_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df1 = df1.drop(['product_uid', 'product_title', 'search_term', 'product_description'], axis=1)\n",
    "x_train, x_test = get_train_test_x(df1)\n",
    "df1 = df1.drop(['dataset'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn import pipeline, grid_search\n",
    "\n",
    "def RMSE(y, y_pred):\n",
    "    return round(mean_squared_error(y, y_pred)**0.5, 3)\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "RMSE_scorer = make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "import sklearn.preprocessing as pp\n",
    "le = pp.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(random_state = 0, verbose = 0)\n",
    "param_grid = {'n_estimators':[30], 'max_features': [10], 'max_depth': [10]}\n",
    "model_rfr = grid_search.GridSearchCV(estimator = rfr, param_grid = param_grid, \n",
    "                                     cv = 2, verbose = 20, scoring=RMSE_scorer, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_model(_model, _x_train, _y_train):\n",
    "    _model.fit(_x_train, _y_train)\n",
    "\n",
    "    print(\"Best parameters found by grid search:\")\n",
    "    print(_model.best_params_)\n",
    "    print(\"Best CV score:\")\n",
    "    print(_model.best_score_)\n",
    "\n",
    "    return _model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_optimized = run_model(model_rfr, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_x_train = x_train\n",
    "ith_e = 0\n",
    "ensemble_iterations = 5\n",
    "\n",
    "while ith_e < ensemble_iterations:\n",
    "    curr_x_train = new_x_train\n",
    "    \n",
    "    ith_e += 1\n",
    "    print 'ith Ensemble Iteration: {}'.format(ith_e)\n",
    "    \n",
    "    model_optimized = run_model(model_rfr, curr_x_train, y_train)\n",
    "    output = model_optimized.predict(curr_x_train)\n",
    "    \n",
    "    new_x_train = np.zeros((curr_x_train.shape[0], curr_x_train.shape[1] + 1))\n",
    "    new_x_train[:,:-1] = curr_x_train\n",
    "    new_x_train[:,-1] = output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "xgbr = XGBRegressor(seed=0)\n",
    "param_grid = {'objective':['reg:linear', 'reg:logistic'], \n",
    "              'n_estimators': [50], 'max_depth': [5], \n",
    "              'learning_rate': [0.01, 0.1]}\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = xgbr, param_grid = param_grid, cv = 2, \n",
    "                                 verbose = 20, scoring=RMSE_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_copy = x_train \n",
    "y_train_copy = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print map(lambda x: round(x, 2), sorted(model_optimized.feature_importances_)[::-1])\n",
    "print df1.columns[np.argsort(model_optimized.feature_importances_)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_best_k_features(k):\n",
    "    return np.argsort(model_optimized.feature_importances_)[::-1][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(predictions_all[0], 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print df1.columns\n",
    "np.around(np.corrcoef(x_train, rowvar=0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(x_train)\n",
    "print(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydotplus\n",
    "dot_data = StringIO() \n",
    "\n",
    "import graphviz\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "for i, _tree in enumerate(rfc.estimators_):\n",
    "    with open('figures/tree_' + str(i) + '.dot', 'w') as dotfile:\n",
    "        dot_data = StringIO() \n",
    "        tree.export_graphviz(_tree, out_file=dot_data)\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_x_train = x_train\n",
    "new_x_test = x_test\n",
    "\n",
    "rfr = RandomForestRegressor(random_state = 0, verbose = 0)\n",
    "param_grid = {'n_estimators':[500], 'max_features': [15,25], 'max_depth': [15,25]}\n",
    "model_rfr = grid_search.GridSearchCV(estimator = rfr, param_grid = param_grid, \n",
    "                                     cv = 2, verbose = 20, scoring=RMSE_scorer, n_jobs=-1)\n",
    "\n",
    "ith_e = 0\n",
    "ensemble_iterations = 1\n",
    "while ith_e < ensemble_iterations:\n",
    "    ith_e += 1\n",
    "    curr_x_train = new_x_train\n",
    "    curr_x_test = new_x_test\n",
    "    \n",
    "    model_optimized = run_model(model_rfr, curr_x_train, y_train)\n",
    "    output_train = model_optimized.predict(curr_x_train)\n",
    "\n",
    "    new_x_train = np.zeros((curr_x_train.shape[0], curr_x_train.shape[1]+1))\n",
    "    new_x_train[:,:-1] = curr_x_train\n",
    "    new_x_train[:,-1] = output_train\n",
    "\n",
    "    output_test = model_optimized.predict(curr_x_test)\n",
    "    new_x_test = np.zeros((curr_x_test.shape[0], curr_x_test.shape[1]+1))\n",
    "    new_x_test[:,:-1] = curr_x_test\n",
    "    new_x_test[:,-1] = output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "with open('results/results_rfr_ensemble.csv', 'wb') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['id', 'relevance'])\n",
    "    for id, pred in zip(df_test.id.values, preds):\n",
    "        csv_writer.writerow([id, pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
