{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Airbnb provided 4 years worth of anonymized data to Kaggle. The goal of competition is to predict the booked destination based on historical web session data. Session data is preprocessed and features are engineered based on given data and XGBoost is used for softmax classification.\n",
    "\n",
    "Based on variable importance analysis, it was determined that month-to-month seasonality led to overfitting. Training using only T-1YR worth of data led to better accuracy. Other ways to improve this model may be to cluster users and destinations or using blended models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from pprint import pprint as pp\n",
    "from collections import defaultdict, Counter\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpldt\n",
    "import warnings\n",
    "import rank_metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_PATHS = {'train_users': 'train_users_2.csv',\n",
    "        'sessions': 'sessions.csv',\n",
    "        'countries': 'countries.csv',\n",
    "        'age_gender_bkts': 'age_gender_bkts.csv',\n",
    "        'test_users': 'test_users.csv'}\n",
    "\n",
    "DATASETS = {}\n",
    "DATASETS['training'] = pd.DataFrame.from_csv(DATASET_PATHS['train_users'], header=0)\n",
    "DATASETS['age_gender_bkts'] = pd.DataFrame.from_csv(DATASET_PATHS['age_gender_bkts'], header=0)\n",
    "DATASETS['countries'] = pd.DataFrame.from_csv(DATASET_PATHS['countries'], header=0)\n",
    "DATASETS['sessions'] = pd.DataFrame.from_csv(DATASET_PATHS['sessions'], header=0)\n",
    "DATASETS['testing'] = pd.DataFrame.from_csv(DATASET_PATHS['test_users'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = DATASETS['training'].copy()\n",
    "df_test = DATASETS['testing'].copy()\n",
    "df_age_gender_buckets = DATASETS['age_gender_bkts']\n",
    "\n",
    "df_test_x = df_test.copy()\n",
    "df_train_y = df_train['country_destination']\n",
    "df_train_x = df_train.drop('country_destination', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess sessions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_datasets(df, group_by_columns, pivot_by, column_length=None, custom_columns=None):\n",
    "    \n",
    "    df = df.reset_index().groupby(group_by_columns).sum().reset_index()\n",
    "    \n",
    "    if column_length:\n",
    "         columns, count = zip(*Counter(df[pivot_by]).most_common()[:column_length])\n",
    "    else:\n",
    "         columns, count = zip(*Counter(df[pivot_by]).most_common())\n",
    "        \n",
    "    columns = list(columns)\n",
    "    if custom_columns:\n",
    "        aggregated_df = df.pivot_table('secs_elapsed', 'user_id', pivot_by)[custom_columns] \n",
    "    else:\n",
    "        aggregated_df = df.pivot_table('secs_elapsed', 'user_id', pivot_by)[columns] \n",
    "    \n",
    "    aggregated_df.fillna(-1, inplace=True)\n",
    "    \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_datasets_w_device(df, group_by_columns, pivot_by, column_length=None, custom_columns=None):\n",
    "    \n",
    "    group_by_columns = group_by_columns + ['device_type']\n",
    "    df = df.reset_index().groupby(group_by_columns + []).sum().reset_index()\n",
    "    \n",
    "    pivot_label = pivot_by + '_device_type'\n",
    "    df[pivot_label] = df[pivot_by] + '_' + df.device_type\n",
    "    \n",
    "    if column_length:\n",
    "        columns, count = zip(*Counter(df[pivot_label]).most_common()[:column_length])\n",
    "    else:\n",
    "        columns, count = zip(*Counter(df[pivot_label]).most_common())\n",
    "        \n",
    "    columns = list(columns)\n",
    "    if custom_columns:\n",
    "        aggregated_df = df.pivot_table('secs_elapsed', 'user_id', pivot_label)[custom_columns] \n",
    "    else:\n",
    "        aggregated_df = df.pivot_table('secs_elapsed', 'user_id', pivot_label)[columns] \n",
    "    \n",
    "    aggregated_df.fillna(-1, inplace=True)\n",
    "    \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device_summary = group_datasets(DATASETS['sessions'].copy(),\n",
    "                                ['user_id','device_type'],\n",
    "                                'device_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_details_summary = group_datasets(DATASETS['sessions'].copy(), \n",
    "                                        ['user_id','action_detail'],\n",
    "                                        'action_detail', column_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions_generic_summary = group_datasets(DATASETS['sessions'].copy(), \n",
    "                                        ['user_id','action'],\n",
    "                                        'action', column_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions_type_columns = ['-unknown-', 'view', 'data', 'click', 'submit', 'message_post', \n",
    "                        'partner_callback', 'booking_request', 'booking_response']\n",
    "actions_type_summary = group_datasets(DATASETS['sessions'].copy(), \n",
    "                                        ['user_id','action_type'],\n",
    "                                        'action_type', custom_columns=actions_type_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>index_x</th>\n",
       "      <th>user_id</th>\n",
       "      <th>-unknown-_x</th>\n",
       "      <th>p3</th>\n",
       "      <th>header_userpic_x</th>\n",
       "      <th>view_search_results</th>\n",
       "      <th>wishlist_content_update</th>\n",
       "      <th>user_profile</th>\n",
       "      <th>change_trip_characteristics</th>\n",
       "      <th>...</th>\n",
       "      <th>Android Phone</th>\n",
       "      <th>iPad Tablet</th>\n",
       "      <th>Android App Unknown Phone/Tablet</th>\n",
       "      <th>Tablet</th>\n",
       "      <th>Linux Desktop</th>\n",
       "      <th>Chromebook</th>\n",
       "      <th>Windows Phone</th>\n",
       "      <th>Blackberry</th>\n",
       "      <th>iPodtouch</th>\n",
       "      <th>Opera Phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00023iyk9l</td>\n",
       "      <td>-1</td>\n",
       "      <td>60596</td>\n",
       "      <td>3198</td>\n",
       "      <td>54791</td>\n",
       "      <td>3515</td>\n",
       "      <td>-1</td>\n",
       "      <td>1447</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0010k6l0om</td>\n",
       "      <td>123033</td>\n",
       "      <td>229119</td>\n",
       "      <td>585</td>\n",
       "      <td>75951</td>\n",
       "      <td>135661</td>\n",
       "      <td>-1</td>\n",
       "      <td>20110</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>001wyh0pz8</td>\n",
       "      <td>22181</td>\n",
       "      <td>10639</td>\n",
       "      <td>-1</td>\n",
       "      <td>158796</td>\n",
       "      <td>-1</td>\n",
       "      <td>1510</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>282965</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0028jgx1x1</td>\n",
       "      <td>489</td>\n",
       "      <td>1027</td>\n",
       "      <td>-1</td>\n",
       "      <td>199802</td>\n",
       "      <td>-1</td>\n",
       "      <td>87089</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>002qnbzfs5</td>\n",
       "      <td>738169</td>\n",
       "      <td>56386</td>\n",
       "      <td>10954</td>\n",
       "      <td>493417</td>\n",
       "      <td>-1</td>\n",
       "      <td>125071</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  index_x     user_id  -unknown-_x      p3  header_userpic_x  \\\n",
       "0      0        0  00023iyk9l           -1   60596              3198   \n",
       "1      1        1  0010k6l0om       123033  229119               585   \n",
       "2      2        2  001wyh0pz8        22181   10639                -1   \n",
       "3      3        3  0028jgx1x1          489    1027                -1   \n",
       "4      4        4  002qnbzfs5       738169   56386             10954   \n",
       "\n",
       "   view_search_results  wishlist_content_update  user_profile  \\\n",
       "0                54791                     3515            -1   \n",
       "1                75951                   135661            -1   \n",
       "2               158796                       -1          1510   \n",
       "3               199802                       -1         87089   \n",
       "4               493417                       -1        125071   \n",
       "\n",
       "   change_trip_characteristics     ...       Android Phone  iPad Tablet  \\\n",
       "0                         1447     ...                  -1           -1   \n",
       "1                        20110     ...                  -1           -1   \n",
       "2                           -1     ...                  -1           -1   \n",
       "3                           -1     ...                  -1           -1   \n",
       "4                           -1     ...                  -1           -1   \n",
       "\n",
       "   Android App Unknown Phone/Tablet  Tablet  Linux Desktop  Chromebook  \\\n",
       "0                                -1      -1             -1          -1   \n",
       "1                                -1      -1             -1          -1   \n",
       "2                            282965      -1             -1          -1   \n",
       "3                                -1      -1             -1          -1   \n",
       "4                                -1      -1             -1          -1   \n",
       "\n",
       "   Windows Phone  Blackberry  iPodtouch  Opera Phone  \n",
       "0             -1          -1         -1           -1  \n",
       "1             -1          -1         -1           -1  \n",
       "2             -1          -1         -1           -1  \n",
       "3             -1          -1         -1           -1  \n",
       "4             -1          -1         -1           -1  \n",
       "\n",
       "[5 rows x 146 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_summary = pd.merge(action_details_summary.reset_index(), actions_type_summary.reset_index(), on='user_id', how='outer')\n",
    "action_summary = pd.merge(action_summary.reset_index(), actions_generic_summary.reset_index(), on='user_id', how='outer')\n",
    "sessions_summary = pd.merge(action_summary.reset_index(), device_summary.reset_index(), on='user_id', how='outer')\n",
    "sessions_summary.fillna(-1, inplace=True)\n",
    "sessions_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing on the main dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_features = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', \n",
    "                        'affiliate_provider', 'first_affiliate_tracked', 'signup_app', \n",
    "                        'first_device_type', 'first_browser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if feature frequencies are similar between training and testing set\n",
    "def print_feature_counter(df_train, df_test):\n",
    "    for f in categorical_features:\n",
    "        print 'feature: {}'.format(f)\n",
    "        print 'train set types: {}'.format(set(df_train[f]))\n",
    "        print 'test set types: {}'.format(set(df_test[f]))\n",
    "        \n",
    "        num_training_records = len(df_train)\n",
    "        num_testing_records = len(df_test)\n",
    "        \n",
    "        print 'train set types: {}'.format(Counter(df_train[f]))\n",
    "        print 'test set types: {} \\n'.format(Counter(df_test[f]))\n",
    "        \n",
    "# print_feature_counter(df_train_x, df_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Age Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_features = ['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records: 213451\n",
      "# of records w/ nan in attribute age: 87990\n",
      "% of records w/ nan in attribute age: 41.2%\n",
      "# of records: 62096\n",
      "# of records w/ nan in attribute age: 28876\n",
      "% of records w/ nan in attribute age: 46.5%\n"
     ]
    }
   ],
   "source": [
    "def how_many_nan(df, f):\n",
    "    print '# of records: {}'.format(len(df))\n",
    "    print '# of records w/ nan in attribute {}: {}'.format(\n",
    "        f, len(df[np.isnan(df[f])]))\n",
    "    print '% of records w/ nan in attribute {}: {}%'.format(\n",
    "        f, round(len(df[np.isnan(df[f])]) / float(len(df[f])) * 100, 1))\n",
    "    \n",
    "how_many_nan(df_train_x, 'age')\n",
    "how_many_nan(df_test_x, 'age') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_x['age'][(df_train_x['age'] <= 18) & (df_train_x['age'] >= 90)] = float('nan') \n",
    "df_test_x['age'][(df_test_x['age'] <= 18) & (df_test_x['age'] >= 90)] = float('nan')\n",
    "\n",
    "df_train_x['is_age_nan'] = 0\n",
    "df_train_x['is_age_nan'][df_train_x['age'].isnull()] = 1\n",
    "df_test_x['is_age_nan'] = 0\n",
    "df_test_x['is_age_nan'][df_test_x['age'].isnull()] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime_features = ['date_account_created', 'timestamp_first_active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import math\n",
    "month_num2abbr = dict((k,v) for k,v in enumerate(calendar.month_abbr))\n",
    "\n",
    "def get_month_abbr(num):\n",
    "    if math.isnan(num):\n",
    "        return float('nan')\n",
    "    \n",
    "    return month_num2abbr[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "import datetime\n",
    "day_of_week_mapper = {0:'M', 1:'T', 2:'W', 3:'R', 4:'F', 5:'S', 6:'U'}\n",
    "us_holidays = holidays.UnitedStates() \n",
    "for dataset in [df_train_x, df_test_x]:\n",
    "    for f in datetime_features:\n",
    "        if f == 'timestamp_first_active':\n",
    "            dataset.loc[:, f] = [datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:8])) \n",
    "                                 for x in dataset.loc[:, f]]\n",
    "        else:\n",
    "            dataset.loc[:, f] = pd.to_datetime(dataset.loc[:, f])\n",
    "        \n",
    "        # mapping month as categorical feature to keep seasonality\n",
    "        dataset[f + '_day_of_week'] = [day_of_week_mapper[x] for x in pd.DatetimeIndex(dataset[f]).dayofweek]\n",
    "        dataset[f + '_day'] = pd.DatetimeIndex(dataset[f]).day\n",
    "        dataset[f + '_month'] = map(get_month_abbr, pd.DatetimeIndex(dataset[f]).month)\n",
    "        dataset[f + '_year'] = pd.DatetimeIndex(dataset[f]).year - min(pd.DatetimeIndex(dataset[f]).year)\n",
    "        dataset[f + '_is_holiday'] = [x in us_holidays for x in pd.DatetimeIndex(dataset[f])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  only select most recent last year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_y = df_train_y[df_train_x.date_account_created_year == 4]\n",
    "df_train_x = df_train_x[df_train_x.date_account_created_year == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_drop_features = ['date_account_created', 'timestamp_first_active', 'date_first_booking']\n",
    "for dataset in [df_train_x, df_test_x]:\n",
    "    dataset.drop(to_drop_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge user account with sessions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to merge training and testing set to ensure \n",
    "# all possible categorical features are encoded\n",
    "\n",
    "df_train_x['source'] = 'train'\n",
    "df_test_x['source'] = 'test'\n",
    "_df = pd.concat([df_train_x, df_test_x])\n",
    "\n",
    "# merge main dataset with sessions action data\n",
    "_df = _df.reset_index()\n",
    "_df = _df.rename(columns = {'id':'user_id'})\n",
    "_df = pd.merge(_df, sessions_summary, how='left', on=['user_id'])\n",
    "_df.fillna(-1, inplace=True)\n",
    "_df = _df.set_index('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_df_encoded = pd.get_dummies(_df)\n",
    "_df_encoded.fillna(-1, inplace=True)\n",
    "\n",
    "df_train_x_encoded = _df_encoded[_df_encoded['source_train'] == 1]\n",
    "df_test_x_encoded = _df_encoded[_df_encoded['source_test'] == 1]\n",
    "    \n",
    "df_train_x_encoded.drop(['source_train', 'source_test'], axis=1, inplace=True)\n",
    "df_test_x_encoded.drop(['source_train', 'source_test'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pp\n",
    "le = pp.LabelEncoder()\n",
    "train_y_encoded = le.fit_transform(df_train_y)\n",
    "\n",
    "x_train = df_train_x_encoded.values\n",
    "x_test = df_test_x_encoded.values\n",
    "y_train = train_y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom NDCG scorer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_ndcg(y_train, predicted_probs):\n",
    "    predicted_labels = []\n",
    "    for predicted_prob in predicted_probs:\n",
    "        predicted_labels.append(np.argsort(predicted_prob)[::-1][:5])\n",
    "\n",
    "    score = 0\n",
    "    for prediction, correct_label in zip(predicted_labels, y_train):\n",
    "        ranks = []\n",
    "        for pred_label in zip(prediction):\n",
    "            if pred_label == correct_label:\n",
    "                ranks.append(1)\n",
    "            else:\n",
    "                ranks.append(0)\n",
    "\n",
    "        score += rank_metrics.ndcg_at_k(ranks, 5, method=1)\n",
    "\n",
    "    return round(score / float(len(y_train)), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0, Accuracy: 0.8572, max_depth: 6, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.75\n",
      "Fold: 1, Accuracy: 0.8563, max_depth: 6, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.75\n",
      "\n",
      "\n",
      "Fold: 0, Accuracy: 0.857, max_depth: 6, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.85\n",
      "Fold: 1, Accuracy: 0.8563, max_depth: 6, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.85\n",
      "\n",
      "\n",
      "Fold: 0, Accuracy: 0.8568, max_depth: 8, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.75\n",
      "Fold: 1, Accuracy: 0.8564, max_depth: 8, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.75\n",
      "\n",
      "\n",
      "Fold: 0, Accuracy: 0.8565, max_depth: 8, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.85\n",
      "Fold: 1, Accuracy: 0.8562, max_depth: 8, learning_rate: 0.2, n_estimators: 20, reg: 7.5, reg_lambda:1, subsample: 0.5, colsample_bytree: 0.85\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## XGBoost\n",
    "## with hyperparameter-tuning\n",
    "## w/ engineered features with all session variables\n",
    "## best set: alpha=0.2, n_estimators=50, score=0.8319\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from itertools import product\n",
    "import pdb\n",
    "\n",
    "kfold = StratifiedKFold(y=y_train, n_folds=2, shuffle=True, random_state=None)\n",
    "\n",
    "params_range = {}\n",
    "params_range['max_depth'] = [6, 8]\n",
    "params_range['learning_rate'] = [0.2] \n",
    "params_range['n_estimators'] = [20] \n",
    "params_range['reg'] = [7.5]\n",
    "params_range['reg_lambda'] = [1]\n",
    "params_range['subsample'] = [0.5]\n",
    "params_range['colsample_bytree'] = [0.75, 0.85]\n",
    "\n",
    "# grid search using k-holdout cross-validation\n",
    "for max_depth, alpha, n_estimators, reg, reg_lambda, subsample, colsample_bytree in product(params_range['max_depth'], params_range['learning_rate'], \n",
    "                                              params_range['n_estimators'], params_range['reg'], params_range['reg_lambda'], \n",
    "                                                   params_range['subsample'], params_range['colsample_bytree']): \n",
    "    \n",
    "    xgb = XGBClassifier(max_depth=max_depth, learning_rate=alpha, n_estimators=n_estimators,\n",
    "                        objective='multi:softprob', subsample=subsample, colsample_bytree=colsample_bytree, \n",
    "                        seed=0, reg_alpha=reg, reg_lambda=reg_lambda, silent=False) \n",
    "        \n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        xgb.fit(x_train[train], y_train[train])\n",
    "            \n",
    "        print 'Fold: {}, Accuracy: {}, max_depth: {}, learning_rate: {}, n_estimators: {}, reg: {}, reg_lambda:{}, subsample: {}, colsample_bytree: {}'.format(\n",
    "            k, score_ndcg(y_train[test], xgb.predict_proba(x_train[test])), \n",
    "            max_depth, alpha, n_estimators, reg, reg_lambda, subsample, colsample_bytree)\n",
    "\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier with optimal parameters and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "xgb = XGBClassifier(max_depth=8, learning_rate=0.2, n_estimators=25, \n",
    "                    objective='multi:softprob', subsample=0.5, \n",
    "                    colsample_bytree=0.5, seed=0, reg_alpha=7.5)\n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "predicted_probs = xgb.predict_proba(x_test)\n",
    "predicted_labels = []\n",
    "for predicted_prob in predicted_probs:\n",
    "    predicted_labels.append(le.inverse_transform(np.argsort(predicted_prob))[::-1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "solution_set = defaultdict(list)\n",
    "\n",
    "predicted_labels_list = predicted_labels\n",
    "with open('results/predictions_xgb_2014.csv', 'wb') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(('id', 'country'))\n",
    "    for id, predicted_labels in zip(df_test_x_encoded.index.values, predicted_labels_list):\n",
    "        for predicted_label, rank in zip(predicted_labels, xrange(5)):      \n",
    "            solution_set[rank].append(predicted_label)\n",
    "            csv_writer.writerow((id, predicted_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
